{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Notebook adapted for bank marketing dataset by DA Feb 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding with the demonstration we'll conduct a few data preparations. Note that Automunge needs following prerequisites to operate:\n",
    "\n",
    "- tabular data in Pandas dataframe or Numpy array format\n",
    "- \"tidy data\" (meaning one feature per column and one observation per row)\n",
    "- if available label column included in the set with column name passed to function as string\n",
    "- a \"train\" data set intended to train a machine learning model and if available a \"test\" set intended to generate predictions from the same model\n",
    "- the train and test data must have consistently formatted data and consistent column headers\n",
    "\n",
    "Ok well introductions complete let's go ahead and manually munge to meet these requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data imports and preliminary munging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First set the file paths\n",
    "\n",
    "train_transaction_filepath = \"bank_marketing_train_classification.csv\"\n",
    "test_transaction_filepath = \"bank_marketing_test_withlabel.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's import them as dataframes. Note both the identify and transaction sets include \n",
    "#a single common column, TransactionID, so we'll use that as an index column to merge\n",
    "\n",
    "train_transaction = pd.read_csv(train_transaction_filepath, error_bad_lines=False, index_col=0)\n",
    "test_transaction = pd.read_csv(test_transaction_filepath, error_bad_lines=False, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'y']\n",
      "\n",
      "tiny_train.shape =  (6590, 21)\n",
      "big_train.shape =  (26360, 21)\n",
      "\n",
      "['age', 'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'y']\n",
      "\n",
      "tiny_test.shape =  (1648, 21)\n",
      "big_test.shape =  (6590, 21)\n"
     ]
    }
   ],
   "source": [
    "#train_transaction.rename(columns={'Unnamed: 0': 'id'}, inplace=True) # DA use index_col=0 on read instead\n",
    "#test_transaction.rename(columns={'Unnamed: 0': 'id'}, inplace=True) # DA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "big_train, tiny_train = train_test_split(train_transaction, test_size=0.2, random_state=42) # small_train 0.002\n",
    "big_test, tiny_test = train_test_split(test_transaction, test_size=0.2, random_state=42) # small_test 0.002\n",
    "\n",
    "print(list(tiny_train))\n",
    "print(\"\")\n",
    "print(\"tiny_train.shape = \", tiny_train.shape)\n",
    "print(\"big_train.shape = \", big_train.shape)\n",
    "print(\"\")\n",
    "print(list(tiny_train))\n",
    "print(\"\")\n",
    "print(\"tiny_test.shape = \", tiny_test.shape)\n",
    "print(\"big_test.shape = \", big_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1482\n",
       "1     166\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DA\n",
    "pd.value_counts(tiny_test['y'].values, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5834\n",
       "1     756\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DA\n",
    "pd.value_counts(tiny_train['y'].values, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automunge install and initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Automunge\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/99/d126fc4e3adf4ca3554acce00b94a4a5e49e4e7da4bda56cd9ec81c608f1/Automunge-2.58-py3-none-any.whl (208kB)\n",
      "\u001b[K     |████████████████████████████████| 215kB 3.4MB/s \n",
      "\u001b[?25hInstalling collected packages: Automunge\n",
      "Successfully installed Automunge-2.58\n"
     ]
    }
   ],
   "source": [
    "#Ok here's where we import our tool with pip install. Note that this step requires  \n",
    "#access to the internet. (Note this import procedure changed with version 2.58.)\n",
    "\n",
    "# ! pip install Automunge\n",
    "\n",
    "# #or to upgrade (we currently roll out upgrades pretty frequently)\n",
    "# ! pip install Automunge --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And then we initialize the class.\n",
    "\n",
    "from Automunge import Automunger\n",
    "am = Automunger.AutoMunge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ok let's give it a shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well at the risk of overwhelming the reader I'm just going to throw out a full application. Basically, we pass the train set and if available a consistently formatted test set to the function and it returns normalized and numerically encoded sets suitable for the direct application of machine learning. The function returns a series of sets (which based on the options selected may be empty), I find it helps to just copy and paste the full range of arguments and returned sets from the documentation for each application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________\n",
      "Begin Feature Importance evaluation\n",
      "\n",
      "_______________\n",
      "Begin Automunge processing\n",
      "\n",
      "error, non integer index passed without columns named\n",
      "error, non integer index passed without columns named\n",
      "evaluating column:  age\n",
      "processing column:  age\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['age_nmbr']\n",
      "\n",
      "evaluating column:  job\n",
      "processing column:  job\n",
      "    root category:  1010\n",
      " returned columns:\n",
      "['job_1010_3', 'job_1010_2', 'job_1010_1', 'job_1010_0']\n",
      "\n",
      "evaluating column:  marital\n",
      "processing column:  marital\n",
      "    root category:  1010\n",
      " returned columns:\n",
      "['marital_1010_0', 'marital_1010_2', 'marital_1010_1']\n",
      "\n",
      "evaluating column:  education\n",
      "processing column:  education\n",
      "    root category:  1010\n",
      " returned columns:\n",
      "['education_1010_3', 'education_1010_1', 'education_1010_2', 'education_1010_0']\n",
      "\n",
      "evaluating column:  default\n",
      "processing column:  default\n",
      "    root category:  1010\n",
      " returned columns:\n",
      "['default_1010_1', 'default_1010_0']\n",
      "\n",
      "evaluating column:  housing\n",
      "processing column:  housing\n",
      "    root category:  1010\n",
      " returned columns:\n",
      "['housing_1010_1', 'housing_1010_0']\n",
      "\n",
      "evaluating column:  loan\n",
      "processing column:  loan\n",
      "    root category:  1010\n",
      " returned columns:\n",
      "['loan_1010_0', 'loan_1010_1']\n",
      "\n",
      "evaluating column:  contact\n",
      "processing column:  contact\n",
      "    root category:  bnry\n",
      " returned columns:\n",
      "['contact_bnry']\n",
      "\n",
      "evaluating column:  month\n",
      "processing column:  month\n",
      "    root category:  1010\n",
      " returned columns:\n",
      "['month_1010_2', 'month_1010_3', 'month_1010_1', 'month_1010_0']\n",
      "\n",
      "evaluating column:  day_of_week\n",
      "processing column:  day_of_week\n",
      "    root category:  1010\n",
      " returned columns:\n",
      "['day_of_week_1010_1', 'day_of_week_1010_2', 'day_of_week_1010_0']\n",
      "\n",
      "evaluating column:  duration\n",
      "processing column:  duration\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['duration_nmbr']\n",
      "\n",
      "evaluating column:  campaign\n",
      "processing column:  campaign\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['campaign_nmbr']\n",
      "\n",
      "evaluating column:  pdays\n",
      "processing column:  pdays\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['pdays_nmbr']\n",
      "\n",
      "evaluating column:  previous\n",
      "processing column:  previous\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['previous_nmbr']\n",
      "\n",
      "evaluating column:  poutcome\n",
      "processing column:  poutcome\n",
      "    root category:  1010\n",
      " returned columns:\n",
      "['poutcome_1010_0', 'poutcome_1010_1']\n",
      "\n",
      "evaluating column:  emp_var_rate\n",
      "processing column:  emp_var_rate\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['emp_var_rate_nmbr']\n",
      "\n",
      "evaluating column:  cons_price_idx\n",
      "processing column:  cons_price_idx\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['cons_price_idx_nmbr']\n",
      "\n",
      "evaluating column:  cons_conf_idx\n",
      "processing column:  cons_conf_idx\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['cons_conf_idx_nmbr']\n",
      "\n",
      "evaluating column:  euribor3m\n",
      "processing column:  euribor3m\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['euribor3m_nmbr']\n",
      "\n",
      "evaluating column:  nr_employed\n",
      "processing column:  nr_employed\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['nr_employed_nmbr']\n",
      "\n",
      "evaluating label column:  y\n",
      "______\n",
      "\n",
      "processing label column:  y\n",
      "    root label category:  bnry\n",
      "\n",
      " returned columns:\n",
      "['y_bnry']\n",
      "\n",
      "______\n",
      "MLinfill infilliterate iteration:  0\n",
      " \n",
      "infill to column:  age_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  job_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  job_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  job_1010_2\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  job_1010_3\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  marital_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  marital_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  marital_1010_2\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  education_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  education_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  education_1010_2\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  education_1010_3\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  default_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  default_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  housing_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  housing_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  loan_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  loan_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  contact_bnry\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  month_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  month_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  month_1010_2\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  month_1010_3\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  day_of_week_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  day_of_week_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  day_of_week_1010_2\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  duration_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  campaign_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  pdays_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  previous_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  poutcome_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  poutcome_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  emp_var_rate_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  cons_price_idx_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  cons_conf_idx_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  euribor3m_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  nr_employed_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "_______________\n",
      "Begin Validation set processing with Postmunge\n",
      "\n",
      "_______________\n",
      "Begin Postmunge processing\n",
      "\n",
      "______\n",
      "\n",
      "processing column:  age\n",
      "    root category:  nmbr\n",
      "\n",
      " returned columns:\n",
      "['age_nmbr']\n",
      "\n",
      "______\n",
      "\n",
      "processing column:  job\n",
      "    root category:  1010\n",
      "\n",
      " returned columns:\n",
      "['job_1010_3', 'job_1010_2', 'job_1010_1', 'job_1010_0']\n",
      "\n",
      "______\n",
      "\n",
      "processing column:  marital\n",
      "    root category:  1010\n",
      "\n",
      " returned columns:\n",
      "['marital_1010_0', 'marital_1010_2', 'marital_1010_1']\n",
      "\n",
      "______\n",
      "\n",
      "processing column:  education\n",
      "    root category:  1010\n",
      "\n",
      " returned columns:\n",
      "['education_1010_3', 'education_1010_1', 'education_1010_2', 'education_1010_0']\n",
      "\n",
      "______\n",
      "\n",
      "processing column:  default\n",
      "    root category:  1010\n",
      "\n",
      " returned columns:\n",
      "['default_1010_1', 'default_1010_0']\n",
      "\n",
      "______\n",
      "\n",
      "processing column:  housing\n",
      "    root category:  1010\n",
      "\n",
      " returned columns:\n",
      "['housing_1010_1', 'housing_1010_0']\n",
      "\n",
      "______\n",
      "\n",
      "processing column:  loan\n",
      "    root category:  1010\n",
      "\n",
      " returned columns:\n",
      "['loan_1010_0', 'loan_1010_1']\n",
      "\n",
      "______\n",
      "\n",
      "processing column:  contact\n",
      "    root category:  bnry\n",
      "\n",
      " returned columns:\n",
      "['contact_bnry']\n",
      "\n",
      "______\n",
      "\n",
      "processing column:  month\n",
      "    root category:  1010\n",
      "\n",
      " returned columns:\n",
      "['month_1010_2', 'month_1010_3', 'month_1010_1', 'month_1010_0']\n",
      "\n",
      "______\n",
      "\n",
      "processing column:  day_of_week\n",
      "    root category:  1010\n",
      "\n",
      " returned columns:\n",
      "['day_of_week_1010_1', 'day_of_week_1010_2', 'day_of_week_1010_0']\n",
      "\n",
      "______\n",
      "\n",
      "processing column:  duration\n",
      "    root category:  nmbr\n",
      "\n",
      " returned columns:\n",
      "['duration_nmbr']\n",
      "\n",
      "______\n",
      "\n",
      "processing column:  campaign\n",
      "    root category:  nmbr\n",
      "\n",
      " returned columns:\n",
      "['campaign_nmbr']\n",
      "\n",
      "______\n",
      "\n",
      "processing column:  pdays\n",
      "    root category:  nmbr\n",
      "\n",
      " returned columns:\n",
      "['pdays_nmbr']\n",
      "\n",
      "______\n",
      "\n",
      "processing column:  previous\n",
      "    root category:  nmbr\n",
      "\n",
      " returned columns:\n",
      "['previous_nmbr']\n",
      "\n",
      "______\n",
      "\n",
      "processing column:  poutcome\n",
      "    root category:  1010\n",
      "\n",
      " returned columns:\n",
      "['poutcome_1010_0', 'poutcome_1010_1']\n",
      "\n",
      "______\n",
      "\n",
      "processing column:  emp_var_rate\n",
      "    root category:  nmbr\n",
      "\n",
      " returned columns:\n",
      "['emp_var_rate_nmbr']\n",
      "\n",
      "______\n",
      "\n",
      "processing column:  cons_price_idx\n",
      "    root category:  nmbr\n",
      "\n",
      " returned columns:\n",
      "['cons_price_idx_nmbr']\n",
      "\n",
      "______\n",
      "\n",
      "processing column:  cons_conf_idx\n",
      "    root category:  nmbr\n",
      "\n",
      " returned columns:\n",
      "['cons_conf_idx_nmbr']\n",
      "\n",
      "______\n",
      "\n",
      "processing column:  euribor3m\n",
      "    root category:  nmbr\n",
      "\n",
      " returned columns:\n",
      "['euribor3m_nmbr']\n",
      "\n",
      "______\n",
      "\n",
      "processing column:  nr_employed\n",
      "    root category:  nmbr\n",
      "\n",
      " returned columns:\n",
      "['nr_employed_nmbr']\n",
      "\n",
      "processing label column:  y\n",
      "    root label category:  bnry\n",
      "\n",
      " returned columns:\n",
      "['y_bnry']\n",
      "\n",
      "______\n",
      "ML infill infilliterate iteration:  0\n",
      " \n",
      "infill to column:  age_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  job_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  job_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  job_1010_2\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  job_1010_3\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  marital_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  marital_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  marital_1010_2\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  education_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  education_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  education_1010_2\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  education_1010_3\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  default_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  default_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  housing_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  housing_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  loan_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  loan_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  contact_bnry\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  month_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  month_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  month_1010_2\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  month_1010_3\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  day_of_week_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  day_of_week_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  day_of_week_1010_2\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  duration_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  campaign_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  pdays_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  previous_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  poutcome_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  poutcome_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  emp_var_rate_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  cons_price_idx_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  cons_conf_idx_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  euribor3m_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  nr_employed_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "Postmunge returned column set: \n",
      "['age_nmbr', 'job_1010_0', 'job_1010_1', 'job_1010_2', 'job_1010_3', 'marital_1010_0', 'marital_1010_1', 'marital_1010_2', 'education_1010_0', 'education_1010_1', 'education_1010_2', 'education_1010_3', 'default_1010_0', 'default_1010_1', 'housing_1010_0', 'housing_1010_1', 'loan_1010_0', 'loan_1010_1', 'contact_bnry', 'month_1010_0', 'month_1010_1', 'month_1010_2', 'month_1010_3', 'day_of_week_1010_0', 'day_of_week_1010_1', 'day_of_week_1010_2', 'duration_nmbr', 'campaign_nmbr', 'pdays_nmbr', 'previous_nmbr', 'poutcome_1010_0', 'poutcome_1010_1', 'emp_var_rate_nmbr', 'cons_price_idx_nmbr', 'cons_conf_idx_nmbr', 'euribor3m_nmbr', 'nr_employed_nmbr']\n",
      "\n",
      "Postmunge returned label column set: \n",
      "['y_bnry']\n",
      "\n",
      "_______________\n",
      "Postmunge Complete\n",
      "\n",
      "versioning serial stamp:\n",
      "_3.20_442175800374_2020-01-31T12:47:38.370672\n",
      "\n",
      "Automunge returned column set: \n",
      "['age_nmbr', 'job_1010_0', 'job_1010_1', 'job_1010_2', 'job_1010_3', 'marital_1010_0', 'marital_1010_1', 'marital_1010_2', 'education_1010_0', 'education_1010_1', 'education_1010_2', 'education_1010_3', 'default_1010_0', 'default_1010_1', 'housing_1010_0', 'housing_1010_1', 'loan_1010_0', 'loan_1010_1', 'contact_bnry', 'month_1010_0', 'month_1010_1', 'month_1010_2', 'month_1010_3', 'day_of_week_1010_0', 'day_of_week_1010_1', 'day_of_week_1010_2', 'duration_nmbr', 'campaign_nmbr', 'pdays_nmbr', 'previous_nmbr', 'poutcome_1010_0', 'poutcome_1010_1', 'emp_var_rate_nmbr', 'cons_price_idx_nmbr', 'cons_conf_idx_nmbr', 'euribor3m_nmbr', 'nr_employed_nmbr']\n",
      "\n",
      "Automunge returned label column set: \n",
      "['y_bnry']\n",
      "\n",
      "_______________\n",
      "Automunge Complete\n",
      "\n",
      "_______________\n",
      "Training feature importance evaluation model\n",
      "\n",
      "_______________\n",
      "Evaluating feature importances\n",
      "\n",
      "_______________\n",
      "Feature Importance results:\n",
      "\n",
      "age_nmbr\n",
      "metric =  0.004137931034482678\n",
      "metric2 =  0.0\n",
      "\n",
      "job_1010_0\n",
      "metric =  0.00505747126436773\n",
      "metric2 =  0.0055172413793103114\n",
      "\n",
      "job_1010_1\n",
      "metric =  0.00505747126436773\n",
      "metric2 =  0.0036781609195402076\n",
      "\n",
      "job_1010_2\n",
      "metric =  0.00505747126436773\n",
      "metric2 =  0.006896551724137834\n",
      "\n",
      "job_1010_3\n",
      "metric =  0.00505747126436773\n",
      "metric2 =  0.003218390804597626\n",
      "\n",
      "marital_1010_0\n",
      "metric =  0.004137931034482678\n",
      "metric2 =  0.0036781609195402076\n",
      "\n",
      "marital_1010_1\n",
      "metric =  0.004137931034482678\n",
      "metric2 =  0.0027586206896551557\n",
      "\n",
      "marital_1010_2\n",
      "metric =  0.004137931034482678\n",
      "metric2 =  0.003218390804597626\n",
      "\n",
      "education_1010_0\n",
      "metric =  0.0018390804597701038\n",
      "metric2 =  0.0018390804597701038\n",
      "\n",
      "education_1010_1\n",
      "metric =  0.0018390804597701038\n",
      "metric2 =  0.0022988505747125743\n",
      "\n",
      "education_1010_2\n",
      "metric =  0.0018390804597701038\n",
      "metric2 =  0.0018390804597701038\n",
      "\n",
      "education_1010_3\n",
      "metric =  0.0018390804597701038\n",
      "metric2 =  0.0027586206896551557\n",
      "\n",
      "default_1010_0\n",
      "metric =  0.0009195402298850519\n",
      "metric2 =  0.0013793103448275223\n",
      "\n",
      "default_1010_1\n",
      "metric =  0.0009195402298850519\n",
      "metric2 =  -0.0013793103448276334\n",
      "\n",
      "housing_1010_0\n",
      "metric =  0.0018390804597701038\n",
      "metric2 =  0.0\n",
      "\n",
      "housing_1010_1\n",
      "metric =  0.0018390804597701038\n",
      "metric2 =  0.0018390804597701038\n",
      "\n",
      "loan_1010_0\n",
      "metric =  0.00045977011494247044\n",
      "metric2 =  0.0\n",
      "\n",
      "loan_1010_1\n",
      "metric =  0.00045977011494247044\n",
      "metric2 =  0.0\n",
      "\n",
      "contact_bnry\n",
      "metric =  0.0045977011494252595\n",
      "metric2 =  0.0\n",
      "\n",
      "month_1010_0\n",
      "metric =  0.0045977011494252595\n",
      "metric2 =  0.0045977011494252595\n",
      "\n",
      "month_1010_1\n",
      "metric =  0.0045977011494252595\n",
      "metric2 =  0.00505747126436773\n",
      "\n",
      "month_1010_2\n",
      "metric =  0.0045977011494252595\n",
      "metric2 =  0.003218390804597626\n",
      "\n",
      "month_1010_3\n",
      "metric =  0.0045977011494252595\n",
      "metric2 =  0.004137931034482678\n",
      "\n",
      "day_of_week_1010_0\n",
      "metric =  0.007816091954022886\n",
      "metric2 =  0.004137931034482678\n",
      "\n",
      "day_of_week_1010_1\n",
      "metric =  0.007816091954022886\n",
      "metric2 =  0.006896551724137834\n",
      "\n",
      "day_of_week_1010_2\n",
      "metric =  0.007816091954022886\n",
      "metric2 =  0.0055172413793103114\n",
      "\n",
      "duration_nmbr\n",
      "metric =  0.04137931034482756\n",
      "metric2 =  0.0\n",
      "\n",
      "campaign_nmbr\n",
      "metric =  0.00045977011494247044\n",
      "metric2 =  0.0\n",
      "\n",
      "pdays_nmbr\n",
      "metric =  0.008275862068965467\n",
      "metric2 =  0.0\n",
      "\n",
      "previous_nmbr\n",
      "metric =  0.0009195402298850519\n",
      "metric2 =  0.0\n",
      "\n",
      "poutcome_1010_0\n",
      "metric =  0.0018390804597701038\n",
      "metric2 =  0.0\n",
      "\n",
      "poutcome_1010_1\n",
      "metric =  0.0018390804597701038\n",
      "metric2 =  0.0018390804597701038\n",
      "\n",
      "emp_var_rate_nmbr\n",
      "metric =  0.003218390804597626\n",
      "metric2 =  0.0\n",
      "\n",
      "cons_price_idx_nmbr\n",
      "metric =  0.004137931034482678\n",
      "metric2 =  0.0\n",
      "\n",
      "cons_conf_idx_nmbr\n",
      "metric =  0.003218390804597626\n",
      "metric2 =  0.0\n",
      "\n",
      "euribor3m_nmbr\n",
      "metric =  0.010114942528735571\n",
      "metric2 =  0.0\n",
      "\n",
      "nr_employed_nmbr\n",
      "metric =  0.008275862068965467\n",
      "metric2 =  0.0\n",
      "\n",
      "_______________\n",
      "Feature Importance evaluation complete\n",
      "\n",
      "_______________\n",
      "Begin Automunge processing\n",
      "\n",
      "error, non integer index passed without columns named\n",
      "error, non integer index passed without columns named\n",
      "evaluating column:  age\n",
      "processing column:  age\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['age_nmbr']\n",
      "\n",
      "evaluating column:  job\n",
      "processing column:  job\n",
      "    root category:  1010\n",
      " returned columns:\n",
      "['job_1010_3', 'job_1010_2', 'job_1010_1', 'job_1010_0']\n",
      "\n",
      "evaluating column:  marital\n",
      "processing column:  marital\n",
      "    root category:  1010\n",
      " returned columns:\n",
      "['marital_1010_0', 'marital_1010_2', 'marital_1010_1']\n",
      "\n",
      "evaluating column:  education\n",
      "processing column:  education\n",
      "    root category:  1010\n",
      " returned columns:\n",
      "['education_1010_3', 'education_1010_1', 'education_1010_2', 'education_1010_0']\n",
      "\n",
      "evaluating column:  default\n",
      "processing column:  default\n",
      "    root category:  1010\n",
      " returned columns:\n",
      "['default_1010_1', 'default_1010_0']\n",
      "\n",
      "evaluating column:  housing\n",
      "processing column:  housing\n",
      "    root category:  1010\n",
      " returned columns:\n",
      "['housing_1010_1', 'housing_1010_0']\n",
      "\n",
      "evaluating column:  loan\n",
      "processing column:  loan\n",
      "    root category:  1010\n",
      " returned columns:\n",
      "['loan_1010_0', 'loan_1010_1']\n",
      "\n",
      "evaluating column:  contact\n",
      "processing column:  contact\n",
      "    root category:  bnry\n",
      " returned columns:\n",
      "['contact_bnry']\n",
      "\n",
      "evaluating column:  month\n",
      "processing column:  month\n",
      "    root category:  1010\n",
      " returned columns:\n",
      "['month_1010_2', 'month_1010_3', 'month_1010_1', 'month_1010_0']\n",
      "\n",
      "evaluating column:  day_of_week\n",
      "processing column:  day_of_week\n",
      "    root category:  1010\n",
      " returned columns:\n",
      "['day_of_week_1010_1', 'day_of_week_1010_2', 'day_of_week_1010_0']\n",
      "\n",
      "evaluating column:  duration\n",
      "processing column:  duration\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['duration_nmbr']\n",
      "\n",
      "evaluating column:  campaign\n",
      "processing column:  campaign\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['campaign_nmbr']\n",
      "\n",
      "evaluating column:  pdays\n",
      "processing column:  pdays\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['pdays_nmbr']\n",
      "\n",
      "evaluating column:  previous\n",
      "processing column:  previous\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['previous_nmbr']\n",
      "\n",
      "evaluating column:  poutcome\n",
      "processing column:  poutcome\n",
      "    root category:  1010\n",
      " returned columns:\n",
      "['poutcome_1010_0', 'poutcome_1010_1']\n",
      "\n",
      "evaluating column:  emp_var_rate\n",
      "processing column:  emp_var_rate\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['emp_var_rate_nmbr']\n",
      "\n",
      "evaluating column:  cons_price_idx\n",
      "processing column:  cons_price_idx\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['cons_price_idx_nmbr']\n",
      "\n",
      "evaluating column:  cons_conf_idx\n",
      "processing column:  cons_conf_idx\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['cons_conf_idx_nmbr']\n",
      "\n",
      "evaluating column:  euribor3m\n",
      "processing column:  euribor3m\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['euribor3m_nmbr']\n",
      "\n",
      "evaluating column:  nr_employed\n",
      "processing column:  nr_employed\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['nr_employed_nmbr']\n",
      "\n",
      "evaluating label column:  y\n",
      "______\n",
      "\n",
      "processing label column:  y\n",
      "    root label category:  bnry\n",
      "\n",
      " returned columns:\n",
      "['y_bnry']\n",
      "\n",
      "______\n",
      "MLinfill infilliterate iteration:  0\n",
      " \n",
      "infill to column:  age_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  job_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  job_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  job_1010_2\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  job_1010_3\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  marital_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  marital_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  marital_1010_2\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  education_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  education_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  education_1010_2\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  education_1010_3\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  default_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  default_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  housing_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  housing_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  loan_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  loan_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  contact_bnry\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  month_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  month_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  month_1010_2\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  month_1010_3\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  day_of_week_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  day_of_week_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  day_of_week_1010_2\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  duration_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  campaign_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  pdays_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  previous_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  poutcome_1010_0\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  poutcome_1010_1\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  emp_var_rate_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  cons_price_idx_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  cons_conf_idx_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  euribor3m_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "infill to column:  nr_employed_nmbr\n",
      "     infill type: stdrdinfill\n",
      "\n",
      "versioning serial stamp:\n",
      "_3.20_213016521756_2020-01-31T12:48:56.605350\n",
      "\n",
      "Automunge returned column set: \n",
      "['age_nmbr', 'job_1010_0', 'job_1010_1', 'job_1010_2', 'job_1010_3', 'marital_1010_0', 'marital_1010_1', 'marital_1010_2', 'education_1010_0', 'education_1010_1', 'education_1010_2', 'education_1010_3', 'default_1010_0', 'default_1010_1', 'housing_1010_0', 'housing_1010_1', 'loan_1010_0', 'loan_1010_1', 'contact_bnry', 'month_1010_0', 'month_1010_1', 'month_1010_2', 'month_1010_3', 'day_of_week_1010_0', 'day_of_week_1010_1', 'day_of_week_1010_2', 'duration_nmbr', 'campaign_nmbr', 'pdays_nmbr', 'previous_nmbr', 'poutcome_1010_0', 'poutcome_1010_1', 'emp_var_rate_nmbr', 'cons_price_idx_nmbr', 'cons_conf_idx_nmbr', 'euribor3m_nmbr', 'nr_employed_nmbr']\n",
      "\n",
      "Automunge returned label column set: \n",
      "['y_bnry']\n",
      "\n",
      "_______________\n",
      "Automunge Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#So first let's just try a generic application with our tiny_train set. Note tiny_train here\n",
    "#represents our train set. If a labels column is available we should include and designate, \n",
    "#and any columns we want to exclude from processing we can designate as \"ID columns\" which\n",
    "#will be carved out and consistently shuffled and partitioned.\n",
    "\n",
    "#Note here we're only demonstrating on the set with the reduced number of features to save time.\n",
    "\n",
    "\n",
    "train, trainID, labels, \\\n",
    "validation1, validationID1, validationlabels1, \\\n",
    "validation2, validationID2, validationlabels2, \\\n",
    "test, testID, testlabels, \\\n",
    "labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
    "featureimportance, postprocess_dict = \\\n",
    "am.automunge(tiny_train, df_test = False, labels_column = 'y', trainID_column = False, \\\n",
    "            testID_column = False, valpercent1=0.0, valpercent2 = 0.0, \\\n",
    "            shuffletrain = False, TrainLabelFreqLevel = False, powertransform = False, \\\n",
    "            binstransform = False, MLinfill = False, infilliterate=1, randomseed = 42, \\\n",
    "            numbercategoryheuristic = 15, pandasoutput = True, NArw_marker = False, \\\n",
    "            featureselection = True, featurepct = 1.0, featuremetric = .02, \\\n",
    "            featuremethod = 'pct', PCAn_components = None, PCAexcl = [], \\\n",
    "            ML_cmnd = {'MLinfill_type':'default', \\\n",
    "                       'MLinfill_cmnd':{'RandomForestClassifier':{}, 'RandomForestRegressor':{}}, \\\n",
    "                       'PCA_type':'default', \\\n",
    "                       'PCA_cmnd':{}}, \\\n",
    "            assigncat = {'mnmx':[], 'mnm2':[], 'mnm3':[], 'mnm4':[], 'mnm5':[], 'mnm6':[], \\\n",
    "                         'nmbr':[], 'nbr2':[], 'nbr3':[], 'MADn':[], 'MAD2':[], 'MAD3':[], \\\n",
    "                         'bins':[], 'bint':[], \\\n",
    "                         'bxcx':[], 'bxc2':[], 'bxc3':[], 'bxc4':[], \\\n",
    "                         'log0':[], 'log1':[], 'pwrs':[], \\\n",
    "                         'bnry':[], 'text':[], 'ordl':[], 'ord2':[], \\\n",
    "                         'date':[], 'dat2':[], 'wkdy':[], 'bshr':[], 'hldy':[], \\\n",
    "                         'excl':[], 'exc2':[], 'exc3':[], 'null':[], 'eval':[]}, \\\n",
    "            assigninfill = {'stdrdinfill':[], 'MLinfill':[], 'zeroinfill':[], 'oneinfill':[], \\\n",
    "                            'adjinfill':[], 'meaninfill':[], 'medianinfill':[]}, \\\n",
    "            transformdict = {}, processdict = {}, \\\n",
    "            printstatus = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6590 entries, 37286 to 32558\n",
      "Data columns (total 37 columns):\n",
      "age_nmbr               6590 non-null float32\n",
      "job_1010_0             6590 non-null int8\n",
      "job_1010_1             6590 non-null int8\n",
      "job_1010_2             6590 non-null int8\n",
      "job_1010_3             6590 non-null int8\n",
      "marital_1010_0         6590 non-null int8\n",
      "marital_1010_1         6590 non-null int8\n",
      "marital_1010_2         6590 non-null int8\n",
      "education_1010_0       6590 non-null int8\n",
      "education_1010_1       6590 non-null int8\n",
      "education_1010_2       6590 non-null int8\n",
      "education_1010_3       6590 non-null int8\n",
      "default_1010_0         6590 non-null int8\n",
      "default_1010_1         6590 non-null int8\n",
      "housing_1010_0         6590 non-null int8\n",
      "housing_1010_1         6590 non-null int8\n",
      "loan_1010_0            6590 non-null int8\n",
      "loan_1010_1            6590 non-null int8\n",
      "contact_bnry           6590 non-null int8\n",
      "month_1010_0           6590 non-null int8\n",
      "month_1010_1           6590 non-null int8\n",
      "month_1010_2           6590 non-null int8\n",
      "month_1010_3           6590 non-null int8\n",
      "day_of_week_1010_0     6590 non-null int8\n",
      "day_of_week_1010_1     6590 non-null int8\n",
      "day_of_week_1010_2     6590 non-null int8\n",
      "duration_nmbr          6590 non-null float32\n",
      "campaign_nmbr          6590 non-null float32\n",
      "pdays_nmbr             6590 non-null float32\n",
      "previous_nmbr          6590 non-null float32\n",
      "poutcome_1010_0        6590 non-null int8\n",
      "poutcome_1010_1        6590 non-null int8\n",
      "emp_var_rate_nmbr      6590 non-null float32\n",
      "cons_price_idx_nmbr    6590 non-null float32\n",
      "cons_conf_idx_nmbr     6590 non-null float32\n",
      "euribor3m_nmbr         6590 non-null float32\n",
      "nr_employed_nmbr       6590 non-null float32\n",
      "dtypes: float32(10), int8(27)\n",
      "memory usage: 482.7 KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = pd.concat([train, labels], axis=1)\n",
    "# new_test = pd.concat([test, test_labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train.to_csv('bank_marketing_train_forML_Automunge.csv')\n",
    "# new_test.to_csv('bank_marketing_test_forML_Automunge.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's going on here is we're calling the function am.automunge and pass the returned sets to a series of objects:\n",
    "```\n",
    "train, trainID, labels, \\\n",
    "validation1, validationID1, validationlabels1, \\\n",
    "validation2, validationID2, validationlabels2, \\\n",
    "test, testID, testlabels, \\\n",
    "labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
    "featureimportance, postprocess_dict = \\\n",
    "```\n",
    "\n",
    "Again we don't have to include all of the parameters when calling the function, but I find it helpful just to copy and paste them all. For example if we just wanted to defer to defaults we could just call:\n",
    "```\n",
    "train, trainID, labels, \\\n",
    "validation1, validationID1, validationlabels1, \\\n",
    "validation2, validationID2, validationlabels2, \\\n",
    "test, testID, testlabels, \\\n",
    "labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
    "featureimportance, postprocess_dict = \\\n",
    "am.automunge(tiny_train)\n",
    "```\n",
    "\n",
    "Those sets returned from the function call are as follows:\n",
    "\n",
    "- __train, trainID, labels__ : these are the sets intended to train a machine learning model. (The ID set is simply any columns we wanted to exclude from transformations comparably partitioned and shuffled)\n",
    "- __validation1, validationID1, validationlabels1__ : these are sets carved out from the train set intended for hyperparameter tuning validation based on the designated validation1 ratio (defaults to 0.0)\n",
    "- __validation2, validationID2, validationlabels2__ : these are sets carved out from the train set intended for final model validation based on the designated validation2 ratio (defaults to 0.0)\n",
    "- __test, testID, testlabels__ : these are the sets derived from any passed test set intended to generate predictions from the machine learning model trained form the train set, consistently processed as the train set\n",
    "- __labelsencoding_dict__ : this is a dictionary which may prove useful for reverse encoding predictions generated from the machine learning model to be trained from the train set\n",
    "- __finalcolumns_train, finalcolumns_test__ : a list of the columns returned from the transformation, may prove useful in case one wants to ensure consistent column labeling which is required for subsequent processing of any future test data\n",
    "- __featureimportance__ : this stores the results of the feature importance evaluation if user elects to conduct\n",
    "- __postprocess_dict__ : this dictionary should be saved as it may be used as an input to the postmunge funciton to consistently process any subsequently available test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a few items of interest from the returned sets.\n",
    "\n",
    "Notice that the returned sets now include a suffix appended to column name. These suffixes identify what type of transformation were performed. Here we see a few different types of suffixes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age_nmbr',\n",
       " 'job_1010_0',\n",
       " 'job_1010_1',\n",
       " 'job_1010_2',\n",
       " 'job_1010_3',\n",
       " 'marital_1010_0',\n",
       " 'marital_1010_1',\n",
       " 'marital_1010_2',\n",
       " 'education_1010_0',\n",
       " 'education_1010_1',\n",
       " 'education_1010_2',\n",
       " 'education_1010_3',\n",
       " 'default_1010_0',\n",
       " 'default_1010_1',\n",
       " 'housing_1010_0',\n",
       " 'housing_1010_1',\n",
       " 'loan_1010_0',\n",
       " 'loan_1010_1',\n",
       " 'contact_bnry',\n",
       " 'month_1010_0',\n",
       " 'month_1010_1',\n",
       " 'month_1010_2',\n",
       " 'month_1010_3',\n",
       " 'day_of_week_1010_0',\n",
       " 'day_of_week_1010_1',\n",
       " 'day_of_week_1010_2',\n",
       " 'duration_nmbr',\n",
       " 'campaign_nmbr',\n",
       " 'pdays_nmbr',\n",
       " 'previous_nmbr',\n",
       " 'poutcome_1010_0',\n",
       " 'poutcome_1010_1',\n",
       " 'emp_var_rate_nmbr',\n",
       " 'cons_price_idx_nmbr',\n",
       " 'cons_conf_idx_nmbr',\n",
       " 'euribor3m_nmbr',\n",
       " 'nr_employed_nmbr']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#suffixes identifying steps of transformation\n",
    "list(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_nmbr</th>\n",
       "      <th>job_1010_0</th>\n",
       "      <th>job_1010_1</th>\n",
       "      <th>job_1010_2</th>\n",
       "      <th>job_1010_3</th>\n",
       "      <th>marital_1010_0</th>\n",
       "      <th>marital_1010_1</th>\n",
       "      <th>marital_1010_2</th>\n",
       "      <th>education_1010_0</th>\n",
       "      <th>education_1010_1</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign_nmbr</th>\n",
       "      <th>pdays_nmbr</th>\n",
       "      <th>previous_nmbr</th>\n",
       "      <th>poutcome_1010_0</th>\n",
       "      <th>poutcome_1010_1</th>\n",
       "      <th>emp_var_rate_nmbr</th>\n",
       "      <th>cons_price_idx_nmbr</th>\n",
       "      <th>cons_conf_idx_nmbr</th>\n",
       "      <th>euribor3m_nmbr</th>\n",
       "      <th>nr_employed_nmbr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12447</th>\n",
       "      <td>-0.867146</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211589</td>\n",
       "      <td>0.196069</td>\n",
       "      <td>-0.359965</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.853047</td>\n",
       "      <td>-0.225669</td>\n",
       "      <td>0.970467</td>\n",
       "      <td>0.782994</td>\n",
       "      <td>0.852924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25489</th>\n",
       "      <td>0.755016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.569394</td>\n",
       "      <td>-5.100290</td>\n",
       "      <td>5.759129</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.748378</td>\n",
       "      <td>2.060937</td>\n",
       "      <td>-2.231219</td>\n",
       "      <td>-1.474193</td>\n",
       "      <td>-2.803102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0.468752</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.569394</td>\n",
       "      <td>-5.116291</td>\n",
       "      <td>1.679733</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.196777</td>\n",
       "      <td>-1.177989</td>\n",
       "      <td>-1.229331</td>\n",
       "      <td>-1.353633</td>\n",
       "      <td>-0.930166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38408</th>\n",
       "      <td>0.755016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.577441</td>\n",
       "      <td>0.196069</td>\n",
       "      <td>-0.359965</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.853047</td>\n",
       "      <td>-0.225669</td>\n",
       "      <td>0.970467</td>\n",
       "      <td>0.785301</td>\n",
       "      <td>0.852924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8815</th>\n",
       "      <td>0.182488</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146217</td>\n",
       "      <td>0.196069</td>\n",
       "      <td>-0.359965</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.853047</td>\n",
       "      <td>1.538976</td>\n",
       "      <td>-0.271003</td>\n",
       "      <td>0.781840</td>\n",
       "      <td>0.852924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age_nmbr  job_1010_0  job_1010_1  job_1010_2  job_1010_3  \\\n",
       "12447 -0.867146           1           0           1           0   \n",
       "25489  0.755016           0           0           0           1   \n",
       "567    0.468752           0           0           0           1   \n",
       "38408  0.755016           0           0           0           1   \n",
       "8815   0.182488           0           0           1           0   \n",
       "\n",
       "       marital_1010_0  marital_1010_1  marital_1010_2  education_1010_0  \\\n",
       "12447               0               1               1                 0   \n",
       "25489               0               1               0                 0   \n",
       "567                 0               1               1                 0   \n",
       "38408               0               1               0                 0   \n",
       "8815                0               1               1                 0   \n",
       "\n",
       "       education_1010_1  ...  campaign_nmbr  pdays_nmbr  previous_nmbr  \\\n",
       "12447                 1  ...      -0.211589    0.196069      -0.359965   \n",
       "25489                 1  ...      -0.569394   -5.100290       5.759129   \n",
       "567                   1  ...      -0.569394   -5.116291       1.679733   \n",
       "38408                 1  ...       1.577441    0.196069      -0.359965   \n",
       "8815                  0  ...       0.146217    0.196069      -0.359965   \n",
       "\n",
       "       poutcome_1010_0  poutcome_1010_1  emp_var_rate_nmbr  \\\n",
       "12447                1                0           0.853047   \n",
       "25489                1                1          -0.748378   \n",
       "567                  1                1          -1.196777   \n",
       "38408                1                0           0.853047   \n",
       "8815                 1                0           0.853047   \n",
       "\n",
       "       cons_price_idx_nmbr  cons_conf_idx_nmbr  euribor3m_nmbr  \\\n",
       "12447            -0.225669            0.970467        0.782994   \n",
       "25489             2.060937           -2.231219       -1.474193   \n",
       "567              -1.177989           -1.229331       -1.353633   \n",
       "38408            -0.225669            0.970467        0.785301   \n",
       "8815              1.538976           -0.271003        0.781840   \n",
       "\n",
       "       nr_employed_nmbr  \n",
       "12447          0.852924  \n",
       "25489         -2.803102  \n",
       "567           -0.930166  \n",
       "38408          0.852924  \n",
       "8815           0.852924  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#And here's what the returned data looks like.\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon inspection:\n",
    "- addr2, card4, and ProductCD both have a series of suffixes which represent the different categories derived from a one-hot-encoding of a categorical set\n",
    "- each of TransactionDT, TransactionAmt, card1, card2, card3, card5, addr1, addr2, dist1 have the suffix 'nmbr' which represents a z-score normalization\n",
    "- card6 has the suffix 'bnry' which represents a binary (0/1) encoding\n",
    "- P_emaildomain has the suffix 'ordl' which represents an ordinal (integer) encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automunge uses suffix appenders to track the steps of transformations. For example, one could assign transformations to a column which resulted in multiple suffix appenders, such as say:\n",
    "column1_bxcx_nmbr\n",
    "Which would represent a column with original header 'column1' upon which was performed two steps of transformation, a box-cox power law transform followed by a z-score normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels\n",
    "\n",
    "When we conducted the transfomation we also desiganted a label column which was included in the set, so let's take a peek at the returned labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y_bnry'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#as you can see the returned values on the labels column are consistently encoded\n",
    "#as were passed\n",
    "labels[list(labels)[0]].unique() # DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bnry': {'y_bnry': {'missing': 0,\n",
       "   1: 1,\n",
       "   0: 0,\n",
       "   'extravalues': [],\n",
       "   'oneratio': 0.11471927162367224,\n",
       "   'zeroratio': 0.8852807283763278}}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note that if or original labels weren't yet binary encoded, we could inspect the \n",
    "#returned labelsencoding_dict object to determine the basis of encoding.\n",
    "\n",
    "#Here we just see that the 1 value originated from values 1, and the 0 value\n",
    "#originated from values 0 - a trivial example, but this could be helpful if\n",
    "#we had passed a column containing values ['cat', 'dog'] for instance.\n",
    "\n",
    "labelsencoding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age_nmbr\n",
      "metric =  0.004137931034482678\n",
      "metric2 =  0.0\n",
      "\n",
      "job_1010_0\n",
      "metric =  0.00505747126436773\n",
      "metric2 =  0.0055172413793103114\n",
      "\n",
      "job_1010_1\n",
      "metric =  0.00505747126436773\n",
      "metric2 =  0.0036781609195402076\n",
      "\n",
      "job_1010_2\n",
      "metric =  0.00505747126436773\n",
      "metric2 =  0.006896551724137834\n",
      "\n",
      "job_1010_3\n",
      "metric =  0.00505747126436773\n",
      "metric2 =  0.003218390804597626\n",
      "\n",
      "marital_1010_0\n",
      "metric =  0.004137931034482678\n",
      "metric2 =  0.0036781609195402076\n",
      "\n",
      "marital_1010_1\n",
      "metric =  0.004137931034482678\n",
      "metric2 =  0.0027586206896551557\n",
      "\n",
      "marital_1010_2\n",
      "metric =  0.004137931034482678\n",
      "metric2 =  0.003218390804597626\n",
      "\n",
      "education_1010_0\n",
      "metric =  0.0018390804597701038\n",
      "metric2 =  0.0018390804597701038\n",
      "\n",
      "education_1010_1\n",
      "metric =  0.0018390804597701038\n",
      "metric2 =  0.0022988505747125743\n",
      "\n",
      "education_1010_2\n",
      "metric =  0.0018390804597701038\n",
      "metric2 =  0.0018390804597701038\n",
      "\n",
      "education_1010_3\n",
      "metric =  0.0018390804597701038\n",
      "metric2 =  0.0027586206896551557\n",
      "\n",
      "default_1010_0\n",
      "metric =  0.0009195402298850519\n",
      "metric2 =  0.0013793103448275223\n",
      "\n",
      "default_1010_1\n",
      "metric =  0.0009195402298850519\n",
      "metric2 =  -0.0013793103448276334\n",
      "\n",
      "housing_1010_0\n",
      "metric =  0.0018390804597701038\n",
      "metric2 =  0.0\n",
      "\n",
      "housing_1010_1\n",
      "metric =  0.0018390804597701038\n",
      "metric2 =  0.0018390804597701038\n",
      "\n",
      "loan_1010_0\n",
      "metric =  0.00045977011494247044\n",
      "metric2 =  0.0\n",
      "\n",
      "loan_1010_1\n",
      "metric =  0.00045977011494247044\n",
      "metric2 =  0.0\n",
      "\n",
      "contact_bnry\n",
      "metric =  0.0045977011494252595\n",
      "metric2 =  0.0\n",
      "\n",
      "month_1010_0\n",
      "metric =  0.0045977011494252595\n",
      "metric2 =  0.0045977011494252595\n",
      "\n",
      "month_1010_1\n",
      "metric =  0.0045977011494252595\n",
      "metric2 =  0.00505747126436773\n",
      "\n",
      "month_1010_2\n",
      "metric =  0.0045977011494252595\n",
      "metric2 =  0.003218390804597626\n",
      "\n",
      "month_1010_3\n",
      "metric =  0.0045977011494252595\n",
      "metric2 =  0.004137931034482678\n",
      "\n",
      "day_of_week_1010_0\n",
      "metric =  0.007816091954022886\n",
      "metric2 =  0.004137931034482678\n",
      "\n",
      "day_of_week_1010_1\n",
      "metric =  0.007816091954022886\n",
      "metric2 =  0.006896551724137834\n",
      "\n",
      "day_of_week_1010_2\n",
      "metric =  0.007816091954022886\n",
      "metric2 =  0.0055172413793103114\n",
      "\n",
      "duration_nmbr\n",
      "metric =  0.04137931034482756\n",
      "metric2 =  0.0\n",
      "\n",
      "campaign_nmbr\n",
      "metric =  0.00045977011494247044\n",
      "metric2 =  0.0\n",
      "\n",
      "pdays_nmbr\n",
      "metric =  0.008275862068965467\n",
      "metric2 =  0.0\n",
      "\n",
      "previous_nmbr\n",
      "metric =  0.0009195402298850519\n",
      "metric2 =  0.0\n",
      "\n",
      "poutcome_1010_0\n",
      "metric =  0.0018390804597701038\n",
      "metric2 =  0.0\n",
      "\n",
      "poutcome_1010_1\n",
      "metric =  0.0018390804597701038\n",
      "metric2 =  0.0018390804597701038\n",
      "\n",
      "emp_var_rate_nmbr\n",
      "metric =  0.003218390804597626\n",
      "metric2 =  0.0\n",
      "\n",
      "cons_price_idx_nmbr\n",
      "metric =  0.004137931034482678\n",
      "metric2 =  0.0\n",
      "\n",
      "cons_conf_idx_nmbr\n",
      "metric =  0.003218390804597626\n",
      "metric2 =  0.0\n",
      "\n",
      "euribor3m_nmbr\n",
      "metric =  0.010114942528735571\n",
      "metric2 =  0.0\n",
      "\n",
      "nr_employed_nmbr\n",
      "metric =  0.008275862068965467\n",
      "metric2 =  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for keys,values in featureimportance.items():\n",
    "    print(keys)\n",
    "    print('metric = ', values['metric'])\n",
    "    print('metric2 = ', values['metric2'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsequent consistent processing with postmunge(.)\n",
    "\n",
    "Another important object returned form the automunge application is what we call the \"postprocess_dict\". In fact, good practice is that we should always save externally any postprocess_dict returned from the application of automunge whose output was used to train a machine learning model. Why? Well using this postprocess_dict object, we can then pass any subsequently available \"test\" data that we want to use to generate predictions from that machine learning model giving fully consistent processing and encoding. Let's demonstrate. \n",
    "\n",
    "When we performed a train_test_split above to derive the \"tiny_train\" set, we also ended up with a bigger set called \"tiny_train_bigger\". Let's try applying the postmunge function to consistently process.\n",
    "\n",
    "Note a few pre-requisites for the appplication of postmunge:\n",
    "\n",
    "- requires passing a postprocess_dict that was dervied from the application of automunge\n",
    "- consistently formatted data as the train set used in the application of automunge from which the postprocess_dict was derived\n",
    "- consistent column labeling as the train set used in the application of automunge from which the postprocess_dict was derived\n",
    "\n",
    "And there we have it, let's demonstrate the postmunge function on the set \"tiny_test\" we prepared above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________\n",
      "Begin Postmunge processing\n",
      "\n",
      "error, non integer index passed without columns named\n",
      "error, different number of original columns in train and test sets\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-797d97c97e99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlabelsencoding_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalcolumns_test\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m am.postmunge(postprocess_dict, tiny_test, testID_column = False, \\\n\u001b[1;32m----> 4\u001b[1;33m              labelscolumn = False, pandasoutput=True, printstatus=True)\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "test, testID, testlabels, \\\n",
    "labelsencoding_dict, finalcolumns_test = \\\n",
    "am.postmunge(postprocess_dict, tiny_test, testID_column = False, \\\n",
    "             labelscolumn = False, pandasoutput=True, printstatus=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_nmbr</th>\n",
       "      <th>age_nmbr</th>\n",
       "      <th>job_1010_0</th>\n",
       "      <th>job_1010_1</th>\n",
       "      <th>job_1010_2</th>\n",
       "      <th>job_1010_3</th>\n",
       "      <th>marital_1010_0</th>\n",
       "      <th>marital_1010_1</th>\n",
       "      <th>marital_1010_2</th>\n",
       "      <th>education_1010_0</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign_nmbr</th>\n",
       "      <th>pdays_nmbr</th>\n",
       "      <th>previous_nmbr</th>\n",
       "      <th>poutcome_1010_0</th>\n",
       "      <th>poutcome_1010_1</th>\n",
       "      <th>emp_var_rate_nmbr</th>\n",
       "      <th>cons_price_idx_nmbr</th>\n",
       "      <th>cons_conf_idx_nmbr</th>\n",
       "      <th>euribor3m_nmbr</th>\n",
       "      <th>nr_employed_nmbr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>-1.079540</td>\n",
       "      <td>0.564173</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.569394</td>\n",
       "      <td>0.196069</td>\n",
       "      <td>-0.359965</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.853047</td>\n",
       "      <td>1.538976</td>\n",
       "      <td>-0.271003</td>\n",
       "      <td>0.781263</td>\n",
       "      <td>0.852924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5968</th>\n",
       "      <td>0.357053</td>\n",
       "      <td>-0.867146</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.569394</td>\n",
       "      <td>0.196069</td>\n",
       "      <td>-0.359965</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.853047</td>\n",
       "      <td>0.593569</td>\n",
       "      <td>-0.467024</td>\n",
       "      <td>0.779533</td>\n",
       "      <td>0.852924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>1.366708</td>\n",
       "      <td>-1.153410</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211589</td>\n",
       "      <td>0.196069</td>\n",
       "      <td>-0.359965</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.660876</td>\n",
       "      <td>0.724923</td>\n",
       "      <td>0.905127</td>\n",
       "      <td>0.723002</td>\n",
       "      <td>0.340113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6676</th>\n",
       "      <td>-1.653569</td>\n",
       "      <td>-1.344253</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.861829</td>\n",
       "      <td>0.196069</td>\n",
       "      <td>-0.359965</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.660876</td>\n",
       "      <td>0.724923</td>\n",
       "      <td>0.905127</td>\n",
       "      <td>0.723002</td>\n",
       "      <td>0.340113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5606</th>\n",
       "      <td>0.091798</td>\n",
       "      <td>-0.676304</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.569394</td>\n",
       "      <td>0.196069</td>\n",
       "      <td>-0.359965</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.853047</td>\n",
       "      <td>0.593569</td>\n",
       "      <td>-0.467024</td>\n",
       "      <td>0.782417</td>\n",
       "      <td>0.852924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id_nmbr  age_nmbr  job_1010_0  job_1010_1  job_1010_2  job_1010_3  \\\n",
       "706  -1.079540  0.564173           1           0           1           0   \n",
       "5968  0.357053 -0.867146           1           0           1           1   \n",
       "1665  1.366708 -1.153410           1           0           0           0   \n",
       "6676 -1.653569 -1.344253           1           0           0           1   \n",
       "5606  0.091798 -0.676304           1           0           1           0   \n",
       "\n",
       "      marital_1010_0  marital_1010_1  marital_1010_2  education_1010_0  ...  \\\n",
       "706                0               1               0                 0  ...   \n",
       "5968               0               1               1                 0  ...   \n",
       "1665               0               1               0                 1  ...   \n",
       "6676               0               1               1                 1  ...   \n",
       "5606               0               1               1                 0  ...   \n",
       "\n",
       "      campaign_nmbr  pdays_nmbr  previous_nmbr  poutcome_1010_0  \\\n",
       "706       -0.569394    0.196069      -0.359965                1   \n",
       "5968      -0.569394    0.196069      -0.359965                1   \n",
       "1665      -0.211589    0.196069      -0.359965                1   \n",
       "6676       0.861829    0.196069      -0.359965                1   \n",
       "5606      -0.569394    0.196069      -0.359965                1   \n",
       "\n",
       "      poutcome_1010_1  emp_var_rate_nmbr  cons_price_idx_nmbr  \\\n",
       "706                 0           0.853047             1.538976   \n",
       "5968                0           0.853047             0.593569   \n",
       "1665                0           0.660876             0.724923   \n",
       "6676                0           0.660876             0.724923   \n",
       "5606                0           0.853047             0.593569   \n",
       "\n",
       "      cons_conf_idx_nmbr  euribor3m_nmbr  nr_employed_nmbr  \n",
       "706            -0.271003        0.781263          0.852924  \n",
       "5968           -0.467024        0.779533          0.852924  \n",
       "1665            0.905127        0.723002          0.340113  \n",
       "6676            0.905127        0.723002          0.340113  \n",
       "5606           -0.467024        0.782417          0.852924  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#And if we're doing our job right then this set should be formatted exaclty like that returned\n",
    "#from automunge, let's take a look.\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looks good! \n",
    "\n",
    "#So if we wanted to generate predictions from a machine learning model trained \n",
    "#on a train set processed with automunge, we now have a way to consistently \n",
    "#prepare data with postmunge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's explore a (few) of the automunge parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok let's take a look at a few of the optional methods available here. First here again is what a full automunge call looks like:\n",
    "\n",
    "```\n",
    "train, trainID, labels, \\\n",
    "validation1, validationID1, validationlabels1, \\\n",
    "validation2, validationID2, validationlabels2, \\\n",
    "test, testID, testlabels, \\\n",
    "labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
    "featureimportance, postprocess_dict = \\\n",
    "am.automunge(df_train, df_test = False, labels_column = False, trainID_column = False, \\\n",
    "            testID_column = False, valpercent1=0.0, valpercent2 = 0.0, \\\n",
    "            shuffletrain = False, TrainLabelFreqLevel = False, powertransform = False, \\\n",
    "            binstransform = False, MLinfill = False, infilliterate=1, randomseed = 42, \\\n",
    "            numbercategoryheuristic = 15, pandasoutput = True, NArw_marker = True, \\\n",
    "            featureselection = False, featurepct = 1.0, featuremetric = .02, \\\n",
    "            featuremethod = 'pct', PCAn_components = None, PCAexcl = [], \\\n",
    "            ML_cmnd = {'MLinfill_type':'default', \\\n",
    "                       'MLinfill_cmnd':{'RandomForestClassifier':{}, 'RandomForestRegressor':{}}, \\\n",
    "                       'PCA_type':'default', \\\n",
    "                       'PCA_cmnd':{}}, \\\n",
    "            assigncat = {'mnmx':[], 'mnm2':[], 'mnm3':[], 'mnm4':[], 'mnm5':[], 'mnm6':[], \\\n",
    "                         'nmbr':[], 'nbr2':[], 'nbr3':[], 'MADn':[], 'MAD2':[], 'MAD3':[], \\\n",
    "                         'bins':[], 'bint':[], \\\n",
    "                         'bxcx':[], 'bxc2':[], 'bxc3':[], 'bxc4':[], \\\n",
    "                         'log0':[], 'log1':[], 'pwrs':[], \\\n",
    "                         'bnry':[], 'text':[], 'ordl':[], 'ord2':[], \\\n",
    "                         'date':[], 'dat2':[], 'wkdy':[], 'bshr':[], 'hldy':[], \\\n",
    "                         'excl':[], 'exc2':[], 'exc3':[], 'null':[], 'eval':[]}\n",
    "            assigninfill = {'stdrdinfill':[], 'MLinfill':[], 'zeroinfill':[], 'oneinfill':[], \\\n",
    "                            'adjinfill':[], 'meaninfill':[], 'medianinfill':[]}, \\\n",
    "            transformdict = {}, processdict = {}, \\\n",
    "            printstatus = True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's just go through these one by one. (This section is kind of diving into the weeds, not required reading)\n",
    "\n",
    "__df_train__ and __df_test__ First note that if we want we can pass two different pandas dataframe sets to automunge, such as might be beneficial if we have one set with labels (a \"train\" set) and one set without (a \"test\" set). Note that the normalization parameters are all derived just from the train set, and applied for consistent processing of any test set if included. Again a prerequisite is that any train and test set must have consistently labeled columns and consistent formated data, with the exception of any designated \"ID\" columns or \"label\" columns which will be carved out and consistenlty shuffled and partitioned. Note too that we can pass these sets with non-integer-range index or even multi column indexes, such that such index columns will be carved out and returned as part of the ID sets, consistently shuffled and partitioned. If we only want to process a train set we can pass the test set as \"False\".\n",
    "\n",
    "__labels_column__ is intended for passing string identifiers of a column that will be treated as labels. Note that some of the methods require the inclusion of labels, such as feature importance evaluation or label frequency levelizer (for oversampling rows with lower frequency labels).\n",
    "\n",
    "__trainID_column__ and testID_column are intended for passing strings or lists of strings identifying columns that will be carved out before processing and consistently shuffled and partitioned. \n",
    "\n",
    "__valpercent1__ and __valpercent2__ parameters are intended as floats between 0-1 that indicate the ratio of the sets that will be carved out for the two validation sets. If shuffle train is activated then the sets will be carved out randomly, else they will be taken from the bottom sequnetial rows of the train set and randomly partitioned between the two validaiton sets. Note that these values default to 0.\n",
    "\n",
    "__shuffletrain__ parameter indicates whether the train set will be (can you guess?) yep you were right the answer is shuffled.\n",
    "\n",
    "__TrainLabelFreqLevel__ parameter indicates whether the train set will have the oversampling method applied where rows with lower frequency labels are copied for more equal distribution of labels, such as might be beneficial for oversampling in the training operation.\n",
    "\n",
    "__powertransform__ parameter indicates whether default numerical coloumn evluation will include an inference of distribution properties to assign between z-score normalization, min-max scaling, or box-cox power law trasnformation. Note this one is still somewhat rough around the edges and we will continue to refine mewthods going forward.\n",
    "\n",
    "__binstransform__ indicates whether defauilt z-score normalizaiton applicaiton will include the develoipment of bins sets identifying a point's placement with respect to number of standard deviations from the mean.\n",
    "\n",
    "__MLinfill__ indicates whether default infill methods will predict infill for missing points using machine learning models trained on the rest of the set in generalizaed and automated fashion. Note that this method benefits from increased scale of data in teh train set, and mmodels derbvied from the train set are used for consistent prediction methods for the test set.\n",
    "\n",
    "__infilliterate__ indicates whether the predictive methods for MLinfill will be iterated by this integer such as may be beneficial for particularily messy data.\n",
    "\n",
    "__randomseed__ seed for randomness for all of the random seeded methods such as predcitive algorithms for ML infill, feature importance, PCA, shuffling, etc\n",
    "\n",
    "__numbercategoryheuristic__ an integer indicating for categorical sets the threshold between processing with one-hot-encoding vs ordinal methods\n",
    "\n",
    "__pandasoutput__ quite simply True means returned sets are pandas dataframes, False means Numpy arrays (defaults to Numpy arrays)\n",
    "\n",
    "__NArw_marker__ indicates whether returned columns will include a derived column indicating rows that were subject to infill (can be identified with the suffix \"NArw\")\n",
    "\n",
    "__featureselection__ indicated whether a feature importance evlauation will be performed (using then shuffle permeation method), note this requires the inclusion of a designated loabels column in the train set. Results are presented in the returned object \"featureimportance\"\n",
    "\n",
    "__featurepct__ if feature selection performed and featuremethod == 'pct', indicates what percent of columns will be retained from the feature importance dimensionality reduction (columns are ranked by importance and the low percent are trimmed). Note that a value of 1.0 means no trimming will be done.\n",
    "\n",
    "__featuremetric__ if feature selection performed and featuremethod == 'metric', indicates what threshold of importance metric will be required for retained columns from the feature importance dimensionality reduction (columns feature importance metrics are derived and those below this threshold are trimmed). Note that a value of 0.0 means no trimming will be done.\n",
    "\n",
    "__feteaturemethod__ accepts values of 'pct' or 'metric' indicates method used for any feature importance dimensionality reduction\n",
    "\n",
    "__PCAn_components__ Triggers PCA dimensionality reduction when != None. Can be a float indicating percent of columns to retain in PCA or an integer indicated number of columns to retain. The tool evaluates whether set is suitable for kernel PCA, sparse PCA, or PCA. Alternatively, a user can assign a desired PCA method in the ML-cmnd['PCA_type']. Note that a value of None means no PCA dimensionality reduction will be performed unless the scale of data is below a heuristic based on the number of features. (A user can also just turn off default PCA with ML-cmnd['PCA_type'])\n",
    "\n",
    "__PCAexcl__ a list of any columns to be excluded from PCA trasnformations\n",
    "\n",
    "__ML_cmnd__ allows a user to pass parameters to the predictive algorithms used in ML infill, feature importance, and PCA (I won't go into full detail here, although note one handy feature is we can tell the algorithm to exlcude boolean columns form PCA which is useful)\n",
    "\n",
    "__assigncat__ allows a user to assign distinct columns to different processing methods, for those columns that they don't want to defer to default automated processing. For example a user could designate columns for min-max scaling instead of z-score, or box-cox power law trasnform, or you know we've got a whole library of methods that we're continueing to build out. These are defined in our READ ME. Simply pass the column header string identifier to the list associated with any of these root categories.\n",
    "\n",
    "__assigninfill__ allows a user to assign disinct columns to different infill methods for missing or improperly formatted data, for those columns that they don't want to defer to default automated infill whi ch could be either standard infill (mean to numerical sets, most common to binary, and boolean identifier to categorical), or ML infill if it was selected. \n",
    "\n",
    "__transformdict__ and __processdict__ allows a user to design custom trees or trasnformations or even custom processing functions such as documented in our essays that no one reads. Once defined a column can be assigned to these methods in the assigncat.\n",
    "\n",
    "__printstatus__ You know, like, prints the status during operation. Self-explanatory!\n",
    "\n",
    "\n",
    "Now we'll demonstrate a few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainID_column, shuffletrain, valpercent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#great well let's try a few of these out. How about the ID columns, let's see what happens when we pass one.\n",
    "#Let's just pick an arbitrary one, TransactionDT\n",
    "\n",
    "train, trainID, labels, \\\n",
    "validation1, validationID1, validationlabels1, \\\n",
    "validation2, validationID2, validationlabels2, \\\n",
    "test, testID, testlabels, \\\n",
    "labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
    "featureimportance, postprocess_dict = \\\n",
    "am.automunge(tiny_train, df_test = False, labels_column = 'isFraud', trainID_column = 'TransactionDT', \\\n",
    "             valpercent1=0.20, shuffletrain = True, pandasoutput=True, printstatus = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6621556</td>\n",
       "      <td>3259400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>12432282</td>\n",
       "      <td>3466055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7308608</td>\n",
       "      <td>3282364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8986607</td>\n",
       "      <td>3349548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7419202</td>\n",
       "      <td>3287303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionDT  TransactionID\n",
       "0        6621556        3259400\n",
       "1       12432282        3466055\n",
       "2        7308608        3282364\n",
       "3        8986607        3349548\n",
       "4        7419202        3287303"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we'll find that the TransactionDT column is missing from the train set, left \n",
    "#unaltered instead in the ID set, paired with the Transaction ID which was put\n",
    "#in the ID set because it was a non-integer range index column (thus if we wanted\n",
    "#to reassign the original index column we could simply copy the TransactionID column\n",
    "#from the ID set back to the processed train set)\n",
    "\n",
    "trainID.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10134372</td>\n",
       "      <td>3388943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6131082</td>\n",
       "      <td>3243230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5529859</td>\n",
       "      <td>3220899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10863840</td>\n",
       "      <td>3416676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>12881202</td>\n",
       "      <td>3480925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionDT  TransactionID\n",
       "0       10134372        3388943\n",
       "1        6131082        3243230\n",
       "2        5529859        3220899\n",
       "3       10863840        3416676\n",
       "4       12881202        3480925"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#note that since our automunge call included a validation ratio, we'll find \n",
    "#a portion of the sets partitioned in the validation sets, here for instance\n",
    "#is the validaiton ID sets \n",
    "\n",
    "#(we'll also find returned sets in the validation1, and validationlabels1)\n",
    "\n",
    "#note that since we activated the shuffletrain option these are randomly\n",
    "#selected from the train set\n",
    "\n",
    "validationID1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrainLabelFreqLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape =  (1182, 37)\n"
     ]
    }
   ],
   "source": [
    "#Let's take a look at TrainLabelFreqLevel, which serves to copy rows such as to\n",
    "#(approximately) levelize the frequency of labels found in the set.\n",
    "\n",
    "#First let's look at the shape of a train set returtned from an automunge\n",
    "#applicaiton without this option selected\n",
    "\n",
    "train, trainID, labels, \\\n",
    "validation1, validationID1, validationlabels1, \\\n",
    "validation2, validationID2, validationlabels2, \\\n",
    "test, testID, testlabels, \\\n",
    "labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
    "featureimportance, postprocess_dict = \\\n",
    "am.automunge(tiny_train, df_test = False, labels_column = 'isFraud', TrainLabelFreqLevel=False, \\\n",
    "             pandasoutput=True, printstatus=False)\n",
    "\n",
    "print(\"train.shape = \", train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape =  (2302, 37)\n"
     ]
    }
   ],
   "source": [
    "#OK now let's try again with the option selected. If there was a material discrepency in label frequency\n",
    "#we should see more rows included in the returned set\n",
    "\n",
    "train, trainID, labels, \\\n",
    "validation1, validationID1, validationlabels1, \\\n",
    "validation2, validationID2, validationlabels2, \\\n",
    "test, testID, testlabels, \\\n",
    "labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
    "featureimportance, postprocess_dict = \\\n",
    "am.automunge(tiny_train, df_test = False, labels_column = 'isFraud', TrainLabelFreqLevel=True, \\\n",
    "             pandasoutput=True, printstatus=False)\n",
    "\n",
    "print(\"train.shape = \", train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# binstransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list(train):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['dist2_nmbr_bint_t<-2',\n",
       " 'dist2_nmbr_bint_t-21',\n",
       " 'dist2_nmbr_bint_t-10',\n",
       " 'dist2_nmbr_bint_t+01',\n",
       " 'dist2_nmbr_bint_t+12',\n",
       " 'dist2_nmbr_bint_t>+2',\n",
       " 'dist1_nmbr_bint_t<-2',\n",
       " 'dist1_nmbr_bint_t-21',\n",
       " 'dist1_nmbr_bint_t-10',\n",
       " 'dist1_nmbr_bint_t+01',\n",
       " 'dist1_nmbr_bint_t+12',\n",
       " 'dist1_nmbr_bint_t>+2',\n",
       " 'addr2_26.0',\n",
       " 'addr2_60.0',\n",
       " 'addr2_87.0',\n",
       " 'addr1_nmbr_bint_t<-2',\n",
       " 'addr1_nmbr_bint_t-21',\n",
       " 'addr1_nmbr_bint_t-10',\n",
       " 'addr1_nmbr_bint_t+01',\n",
       " 'addr1_nmbr_bint_t+12',\n",
       " 'addr1_nmbr_bint_t>+2',\n",
       " 'card5_nmbr_bint_t<-2',\n",
       " 'card5_nmbr_bint_t-21',\n",
       " 'card5_nmbr_bint_t-10',\n",
       " 'card5_nmbr_bint_t+01',\n",
       " 'card5_nmbr_bint_t+12',\n",
       " 'card5_nmbr_bint_t>+2',\n",
       " 'card4_american express',\n",
       " 'card4_discover',\n",
       " 'card4_mastercard',\n",
       " 'card4_visa',\n",
       " 'card3_nmbr_bint_t<-2',\n",
       " 'card3_nmbr_bint_t-21',\n",
       " 'card3_nmbr_bint_t-10',\n",
       " 'card3_nmbr_bint_t+01',\n",
       " 'card3_nmbr_bint_t+12',\n",
       " 'card3_nmbr_bint_t>+2',\n",
       " 'card2_nmbr_bint_t<-2',\n",
       " 'card2_nmbr_bint_t-21',\n",
       " 'card2_nmbr_bint_t-10',\n",
       " 'card2_nmbr_bint_t+01',\n",
       " 'card2_nmbr_bint_t+12',\n",
       " 'card2_nmbr_bint_t>+2',\n",
       " 'card1_nmbr_bint_t<-2',\n",
       " 'card1_nmbr_bint_t-21',\n",
       " 'card1_nmbr_bint_t-10',\n",
       " 'card1_nmbr_bint_t+01',\n",
       " 'card1_nmbr_bint_t+12',\n",
       " 'card1_nmbr_bint_t>+2',\n",
       " 'ProductCD_C',\n",
       " 'ProductCD_H',\n",
       " 'ProductCD_R',\n",
       " 'ProductCD_S',\n",
       " 'ProductCD_W',\n",
       " 'TransactionAmt_nmbr_bint_t<-2',\n",
       " 'TransactionAmt_nmbr_bint_t-21',\n",
       " 'TransactionAmt_nmbr_bint_t-10',\n",
       " 'TransactionAmt_nmbr_bint_t+01',\n",
       " 'TransactionAmt_nmbr_bint_t+12',\n",
       " 'TransactionAmt_nmbr_bint_t>+2',\n",
       " 'TransactionDT_nmbr_bint_t<-2',\n",
       " 'TransactionDT_nmbr_bint_t-21',\n",
       " 'TransactionDT_nmbr_bint_t-10',\n",
       " 'TransactionDT_nmbr_bint_t+01',\n",
       " 'TransactionDT_nmbr_bint_t+12',\n",
       " 'TransactionDT_nmbr_bint_t>+2',\n",
       " 'TransactionDT_NArw',\n",
       " 'TransactionDT_nmbr',\n",
       " 'TransactionAmt_NArw',\n",
       " 'TransactionAmt_nmbr',\n",
       " 'ProductCD_NArw',\n",
       " 'card1_NArw',\n",
       " 'card1_nmbr',\n",
       " 'card2_NArw',\n",
       " 'card2_nmbr',\n",
       " 'card3_NArw',\n",
       " 'card3_nmbr',\n",
       " 'card4_NArw',\n",
       " 'card5_NArw',\n",
       " 'card5_nmbr',\n",
       " 'card6_NArw',\n",
       " 'card6_bnry',\n",
       " 'addr1_NArw',\n",
       " 'addr1_nmbr',\n",
       " 'addr2_NArw',\n",
       " 'dist1_NArw',\n",
       " 'dist1_nmbr',\n",
       " 'dist2_NArw',\n",
       " 'dist2_nmbr',\n",
       " 'P_emaildomain_NArw',\n",
       " 'P_emaildomain_ordl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#binstransform just means that default numerical sets will include an additional set of bins identifying\n",
    "#number of standard deviations from the mean. We have to be careful with this one if we don't have a lot\n",
    "#of data as it adds a fair bit of dimensionality\n",
    "\n",
    "train, trainID, labels, \\\n",
    "validation1, validationID1, validationlabels1, \\\n",
    "validation2, validationID2, validationlabels2, \\\n",
    "test, testID, testlabels, \\\n",
    "labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
    "featureimportance, postprocess_dict = \\\n",
    "am.automunge(tiny_train, df_test = False, labels_column = 'isFraud', binstransform=True, \\\n",
    "             pandasoutput=True, printstatus=False)\n",
    "\n",
    "print(\"list(train):\")\n",
    "list(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so the interpretation should be for columns with suffix including \"bint\" that indicates \n",
    "#bins for number fo standard deviations from the mean. For example, nmbr_bint_t+01\n",
    "#would indicated values between mean to +1 standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLinfill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.head()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>addr2_26.0</th>\n",
       "      <th>addr2_60.0</th>\n",
       "      <th>addr2_87.0</th>\n",
       "      <th>card4_american express</th>\n",
       "      <th>card4_discover</th>\n",
       "      <th>card4_mastercard</th>\n",
       "      <th>card4_visa</th>\n",
       "      <th>ProductCD_C</th>\n",
       "      <th>ProductCD_H</th>\n",
       "      <th>ProductCD_R</th>\n",
       "      <th>...</th>\n",
       "      <th>card6_bnry</th>\n",
       "      <th>addr1_NArw</th>\n",
       "      <th>addr1_nmbr</th>\n",
       "      <th>addr2_NArw</th>\n",
       "      <th>dist1_NArw</th>\n",
       "      <th>dist1_nmbr</th>\n",
       "      <th>dist2_NArw</th>\n",
       "      <th>dist2_nmbr</th>\n",
       "      <th>P_emaildomain_NArw</th>\n",
       "      <th>P_emaildomain_ordl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.025535</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.977745</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.523147</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.255207</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.495666</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.084023</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   addr2_26.0  addr2_60.0  addr2_87.0  card4_american express  card4_discover  \\\n",
       "0           0           0           1                       0               0   \n",
       "1           0           0           1                       0               0   \n",
       "2           0           0           1                       0               0   \n",
       "3           0           0           0                       0               0   \n",
       "4           0           0           1                       0               0   \n",
       "\n",
       "   card4_mastercard  card4_visa  ProductCD_C  ProductCD_H  ProductCD_R  ...  \\\n",
       "0                 1           0            0            0            0  ...   \n",
       "1                 0           1            0            0            0  ...   \n",
       "2                 1           0            0            0            0  ...   \n",
       "3                 0           1            1            0            0  ...   \n",
       "4                 0           1            0            0            0  ...   \n",
       "\n",
       "   card6_bnry  addr1_NArw  addr1_nmbr  addr2_NArw  dist1_NArw  dist1_nmbr  \\\n",
       "0           0           0    1.025535           0           1    0.000000   \n",
       "1           0           0    1.977745           0           0   -0.523147   \n",
       "2           0           0    0.255207           0           0   -0.495666   \n",
       "3           0           1    0.000000           1           1    0.000000   \n",
       "4           0           0    0.084023           0           1    0.000000   \n",
       "\n",
       "   dist2_NArw  dist2_nmbr  P_emaildomain_NArw  P_emaildomain_ordl  \n",
       "0           1         0.0                   0                   2  \n",
       "1           1         0.0                   0                  30  \n",
       "2           1         0.0                   0                  10  \n",
       "3           1         0.0                   0                  10  \n",
       "4           1         0.0                   0                  11  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#So MLinfill changes the default infill method from standardinfill (which means mean for \n",
    "#numerical sets, most common for binary, and boolean marker for categorical), to a predictive\n",
    "#method in which a machine learning model is trained for each column to predict infill based\n",
    "#on properties of the rest of the set. This one's pretty neat, but caution that it performs \n",
    "#better with more data as you would expect.\n",
    "\n",
    "#Let's demonstrate, first here's an applicaiton without MLinfill, we'll turn on the NArws option\n",
    "#to output an identifier of rows subject to infill\n",
    "\n",
    "train, trainID, labels, \\\n",
    "validation1, validationID1, validationlabels1, \\\n",
    "validation2, validationID2, validationlabels2, \\\n",
    "test, testID, testlabels, \\\n",
    "labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
    "featureimportance, postprocess_dict = \\\n",
    "am.automunge(tiny_train, df_test = False, labels_column = 'isFraud', MLinfill=False, \\\n",
    "             NArw_marker=True, pandasoutput=True, printstatus=False)\n",
    "\n",
    "print(\"train.head()\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist1_nmbr</th>\n",
       "      <th>dist1_NArw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.523147</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.495666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dist1_nmbr  dist1_NArw\n",
       "0    0.000000           1\n",
       "1   -0.523147           0\n",
       "2   -0.495666           0\n",
       "3    0.000000           1\n",
       "4    0.000000           1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#So upon inspection it looks like we had a few infill points on\n",
    "#columns originating from dist1 (as identified by the NArw columns)\n",
    "#so let's focus on that\n",
    "\n",
    "#As you can see the plug value here is just the mean which for a \n",
    "#z-score normalized set is 0\n",
    "\n",
    "columns = ['dist1_nmbr', 'dist1_NArw']\n",
    "train[columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train[columns].head()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist1_nmbr</th>\n",
       "      <th>dist1_NArw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.486171</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.523147</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.495666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.146145</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.364578</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dist1_nmbr  dist1_NArw\n",
       "0    1.486171           1\n",
       "1   -0.523147           0\n",
       "2   -0.495666           0\n",
       "3   -0.146145           1\n",
       "4   -0.364578           1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now let's try with MLinfill\n",
    "\n",
    "train, trainID, labels, \\\n",
    "validation1, validationID1, validationlabels1, \\\n",
    "validation2, validationID2, validationlabels2, \\\n",
    "test, testID, testlabels, \\\n",
    "labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
    "featureimportance, postprocess_dict = \\\n",
    "am.automunge(tiny_train, df_test = False, labels_column = 'isFraud', MLinfill=True, \\\n",
    "             NArw_marker=True, pandasoutput=True, printstatus=False)\n",
    "\n",
    "print(\"train[columns].head()\")\n",
    "train[columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As you can see the method predicted a unique infill value to each row subject to infill\n",
    "#(as identified by the NArw column). We didn't include a lot of data with this small demonstration\n",
    "#set, so I expect the accuracy of this method would improve with a bigger set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numbercategoryheuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique values in P_emaildomain column pre-processing\n",
      "1182\n"
     ]
    }
   ],
   "source": [
    "# numbercategoryheuristic just changes the threshold for number of unique values in a categorical set\n",
    "#between processing a categorical set via one-hot encoding or ordinal processing (sequential integer encoding)\n",
    "\n",
    "#for example consiter the returned column for the email domain set in the data, if we look above we see the\n",
    "#set was processed as ordinal, let's see why\n",
    "\n",
    "print(\"number of unique values in P_emaildomain column pre-processing\")\n",
    "print(len(train['P_emaildomain_ordl']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So yeah looks like that entry has a unique entry per row, so really not really a good candidate for inclusion at\n",
    "#all, this might be better served carved out into the ID set until such time as we can extract some info from it\n",
    "#prior to processing. But the poitn is if we had set numbercategoryheuristic to 1478 instead of 15 we would have \n",
    "#derived 1477 one-hot-encoded columns from this set which obviosuly would be an issue for this scale of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pandasoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(train)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#pandasoutput just tells whether to return pandas dataframe or numpy arrays (defaults to numpy which\n",
    "#is a more universal elligible input to the different machine learning frameworks)\n",
    "\n",
    "train, trainID, labels, \\\n",
    "validation1, validationID1, validationlabels1, \\\n",
    "validation2, validationID2, validationlabels2, \\\n",
    "test, testID, testlabels, \\\n",
    "labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
    "featureimportance, postprocess_dict = \\\n",
    "am.automunge(tiny_train, df_test = False, labels_column = 'isFraud',  \\\n",
    "             pandasoutput=False, NArw_marker = False, printstatus=False)\n",
    "\n",
    "print(\"type(train)\")\n",
    "print(type(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalcolumns_train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['addr2_26.0',\n",
       " 'addr2_60.0',\n",
       " 'addr2_87.0',\n",
       " 'card4_american express',\n",
       " 'card4_discover',\n",
       " 'card4_mastercard',\n",
       " 'card4_visa',\n",
       " 'ProductCD_C',\n",
       " 'ProductCD_H',\n",
       " 'ProductCD_R',\n",
       " 'ProductCD_S',\n",
       " 'ProductCD_W',\n",
       " 'TransactionDT_nmbr',\n",
       " 'TransactionAmt_nmbr',\n",
       " 'card1_nmbr',\n",
       " 'card2_nmbr',\n",
       " 'card3_nmbr',\n",
       " 'card5_nmbr',\n",
       " 'card6_bnry',\n",
       " 'addr1_nmbr',\n",
       " 'dist1_nmbr',\n",
       " 'dist2_nmbr',\n",
       " 'P_emaildomain_ordl']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#note that if we return numpy arrays and want to view the column headers \n",
    "#(which remember track the steps of transofmations in their suffix appenders)\n",
    "#good news that's available in the returned finalcolumns_train\n",
    "print(\"finalcolumns_train\")\n",
    "finalcolumns_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(train)\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#or with pandasoutput = True\n",
    "\n",
    "train, trainID, labels, \\\n",
    "validation1, validationID1, validationlabels1, \\\n",
    "validation2, validationID2, validationlabels2, \\\n",
    "test, testID, testlabels, \\\n",
    "labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
    "featureimportance, postprocess_dict = \\\n",
    "am.automunge(tiny_train, df_test = False, labels_column = 'isFraud',  \\\n",
    "             pandasoutput=True, NArw_marker = True, printstatus=False)\n",
    "\n",
    "print(\"type(train)\")\n",
    "print(type(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NArw_marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list(train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['addr2_26.0',\n",
       " 'addr2_60.0',\n",
       " 'addr2_87.0',\n",
       " 'card4_american express',\n",
       " 'card4_discover',\n",
       " 'card4_mastercard',\n",
       " 'card4_visa',\n",
       " 'ProductCD_C',\n",
       " 'ProductCD_H',\n",
       " 'ProductCD_R',\n",
       " 'ProductCD_S',\n",
       " 'ProductCD_W',\n",
       " 'TransactionDT_nmbr',\n",
       " 'TransactionAmt_nmbr',\n",
       " 'card1_nmbr',\n",
       " 'card2_nmbr',\n",
       " 'card3_nmbr',\n",
       " 'card5_nmbr',\n",
       " 'card6_bnry',\n",
       " 'addr1_nmbr',\n",
       " 'dist1_nmbr',\n",
       " 'dist2_nmbr',\n",
       " 'P_emaildomain_ordl']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The NArw marker helpfully outputs from each column a marker indicating what rows were\n",
    "#subject to infill. Let's quickly demonstrate. First here again are the returned columns\n",
    "#without this feature activated.\n",
    "\n",
    "train, trainID, labels, \\\n",
    "validation1, validationID1, validationlabels1, \\\n",
    "validation2, validationID2, validationlabels2, \\\n",
    "test, testID, testlabels, \\\n",
    "labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
    "featureimportance, postprocess_dict = \\\n",
    "am.automunge(tiny_train, df_test = False, labels_column = 'isFraud', \\\n",
    "             NArw_marker=False, pandasoutput=True, printstatus=False)\n",
    "\n",
    "print(\"list(train)\")\n",
    "list(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list(train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['addr2_26.0',\n",
       " 'addr2_60.0',\n",
       " 'addr2_87.0',\n",
       " 'card4_american express',\n",
       " 'card4_discover',\n",
       " 'card4_mastercard',\n",
       " 'card4_visa',\n",
       " 'ProductCD_C',\n",
       " 'ProductCD_H',\n",
       " 'ProductCD_R',\n",
       " 'ProductCD_S',\n",
       " 'ProductCD_W',\n",
       " 'TransactionDT_NArw',\n",
       " 'TransactionDT_nmbr',\n",
       " 'TransactionAmt_NArw',\n",
       " 'TransactionAmt_nmbr',\n",
       " 'ProductCD_NArw',\n",
       " 'card1_NArw',\n",
       " 'card1_nmbr',\n",
       " 'card2_NArw',\n",
       " 'card2_nmbr',\n",
       " 'card3_NArw',\n",
       " 'card3_nmbr',\n",
       " 'card4_NArw',\n",
       " 'card5_NArw',\n",
       " 'card5_nmbr',\n",
       " 'card6_NArw',\n",
       " 'card6_bnry',\n",
       " 'addr1_NArw',\n",
       " 'addr1_nmbr',\n",
       " 'addr2_NArw',\n",
       " 'dist1_NArw',\n",
       " 'dist1_nmbr',\n",
       " 'dist2_NArw',\n",
       " 'dist2_nmbr',\n",
       " 'P_emaildomain_NArw',\n",
       " 'P_emaildomain_ordl']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now with NArw_marker turned on.\n",
    "\n",
    "train, trainID, labels, \\\n",
    "validation1, validationID1, validationlabels1, \\\n",
    "validation2, validationID2, validationlabels2, \\\n",
    "test, testID, testlabels, \\\n",
    "labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
    "featureimportance, postprocess_dict = \\\n",
    "am.automunge(tiny_train, df_test = False, labels_column = 'isFraud', \\\n",
    "             NArw_marker=True, pandasoutput=True, printstatus=False)\n",
    "\n",
    "print(\"list(train)\")\n",
    "list(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist1_nmbr</th>\n",
       "      <th>dist1_NArw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.486171</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.523147</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.495666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.146145</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.364578</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dist1_nmbr  dist1_NArw\n",
       "0    1.486171           1\n",
       "1   -0.523147           0\n",
       "2   -0.495666           0\n",
       "3   -0.146145           1\n",
       "4   -0.364578           1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#If we inspect one of these we'll see a marker for what rows were subject to infill\n",
    "#(actually already did this a few cells ago but just to be complete)\n",
    "\n",
    "columns = ['dist1_nmbr', 'dist1_NArw']\n",
    "train[columns].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# featureselection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#featureselection performs a feature importance evaluation with the permutaion method. \n",
    "#(basically trains a machine learning model, and then measures impact to accuaracy \n",
    "#after randomly shuffling each feature)\n",
    "\n",
    "#Let's try it out. Note that this method requires the inclusion of a labels column.\n",
    "\n",
    "train, trainID, labels, \\\n",
    "validation1, validationID1, validationlabels1, \\\n",
    "validation2, validationID2, validationlabels2, \\\n",
    "test, testID, testlabels, \\\n",
    "labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
    "featureimportance, postprocess_dict = \\\n",
    "am.automunge(tiny_train, df_test = False, labels_column = 'isFraud', NArw_marker=False, \\\n",
    "             featureselection=True, pandasoutput=True, printstatus=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addr2_26.0\n",
      "shuffleaccuracy =  0.9769820971867008\n",
      "baseaccuracy =  0.9744245524296675\n",
      "metric =  -0.002557544757033292\n",
      "metric2 =  -0.002557544757033292\n",
      "\n",
      "addr2_60.0\n",
      "shuffleaccuracy =  0.9769820971867008\n",
      "baseaccuracy =  0.9744245524296675\n",
      "metric =  -0.002557544757033292\n",
      "metric2 =  -0.002557544757033292\n",
      "\n",
      "addr2_87.0\n",
      "shuffleaccuracy =  0.9769820971867008\n",
      "baseaccuracy =  0.9744245524296675\n",
      "metric =  -0.002557544757033292\n",
      "metric2 =  0.0\n",
      "\n",
      "card4_american express\n",
      "shuffleaccuracy =  0.9769820971867008\n",
      "baseaccuracy =  0.9744245524296675\n",
      "metric =  -0.002557544757033292\n",
      "metric2 =  -0.002557544757033292\n",
      "\n",
      "card4_discover\n",
      "shuffleaccuracy =  0.9769820971867008\n",
      "baseaccuracy =  0.9744245524296675\n",
      "metric =  -0.002557544757033292\n",
      "metric2 =  -0.002557544757033292\n",
      "\n",
      "card4_mastercard\n",
      "shuffleaccuracy =  0.9769820971867008\n",
      "baseaccuracy =  0.9744245524296675\n",
      "metric =  -0.002557544757033292\n",
      "metric2 =  0.0\n",
      "\n",
      "card4_visa\n",
      "shuffleaccuracy =  0.9769820971867008\n",
      "baseaccuracy =  0.9744245524296675\n",
      "metric =  -0.002557544757033292\n",
      "metric2 =  0.0\n",
      "\n",
      "ProductCD_C\n",
      "shuffleaccuracy =  0.9769820971867008\n",
      "baseaccuracy =  0.9744245524296675\n",
      "metric =  -0.002557544757033292\n",
      "metric2 =  0.0\n",
      "\n",
      "ProductCD_H\n",
      "shuffleaccuracy =  0.9769820971867008\n",
      "baseaccuracy =  0.9744245524296675\n",
      "metric =  -0.002557544757033292\n",
      "metric2 =  -0.002557544757033292\n",
      "\n",
      "ProductCD_R\n",
      "shuffleaccuracy =  0.9769820971867008\n",
      "baseaccuracy =  0.9744245524296675\n",
      "metric =  -0.002557544757033292\n",
      "metric2 =  -0.002557544757033292\n",
      "\n",
      "ProductCD_S\n",
      "shuffleaccuracy =  0.9769820971867008\n",
      "baseaccuracy =  0.9744245524296675\n",
      "metric =  -0.002557544757033292\n",
      "metric2 =  -0.002557544757033292\n",
      "\n",
      "ProductCD_W\n",
      "shuffleaccuracy =  0.9769820971867008\n",
      "baseaccuracy =  0.9744245524296675\n",
      "metric =  -0.002557544757033292\n",
      "metric2 =  -0.002557544757033292\n",
      "\n",
      "TransactionDT_nmbr\n",
      "shuffleaccuracy =  0.9744245524296675\n",
      "baseaccuracy =  0.9744245524296675\n",
      "metric =  0.0\n",
      "metric2 =  0.0\n",
      "\n",
      "TransactionAmt_nmbr\n",
      "shuffleaccuracy =  0.9744245524296675\n",
      "baseaccuracy =  0.9744245524296675\n",
      "metric =  0.0\n",
      "metric2 =  0.0\n",
      "\n",
      "card1_nmbr\n",
      "shuffleaccuracy =  0.9718670076726342\n",
      "baseaccuracy =  0.9744245524296675\n",
      "metric =  0.002557544757033292\n",
      "metric2 =  0.0\n",
      "\n",
      "card2_nmbr\n",
      "shuffleaccuracy =  0.9769820971867008\n",
      "baseaccuracy =  0.9744245524296675\n",
      "metric =  -0.002557544757033292\n",
      "metric2 =  0.0\n",
      "\n",
      "card3_nmbr\n",
      "shuffleaccuracy =  0.9769820971867008\n",
      "baseaccuracy =  0.9744245524296675\n",
      "metric =  -0.002557544757033292\n",
      "metric2 =  0.0\n",
      "\n",
      "card5_nmbr\n",
      "shuffleaccuracy =  0.9744245524296675\n",
      "baseaccuracy =  0.9744245524296675\n",
      "metric =  0.0\n",
      "metric2 =  0.0\n",
      "\n",
      "card6_bnry\n",
      "shuffleaccuracy =  0.9769820971867008\n",
      "baseaccuracy =  0.9744245524296675\n",
      "metric =  -0.002557544757033292\n",
      "metric2 =  0.0\n",
      "\n",
      "addr1_nmbr\n",
      "shuffleaccuracy =  0.9769820971867008\n",
      "baseaccuracy =  0.9744245524296675\n",
      "metric =  -0.002557544757033292\n",
      "metric2 =  0.0\n",
      "\n",
      "dist1_nmbr\n",
      "shuffleaccuracy =  0.9744245524296675\n",
      "baseaccuracy =  0.9744245524296675\n",
      "metric =  0.0\n",
      "metric2 =  0.0\n",
      "\n",
      "dist2_nmbr\n",
      "shuffleaccuracy =  0.9744245524296675\n",
      "baseaccuracy =  0.9744245524296675\n",
      "metric =  0.0\n",
      "metric2 =  0.0\n",
      "\n",
      "P_emaildomain_ordl\n",
      "shuffleaccuracy =  0.9744245524296675\n",
      "baseaccuracy =  0.9744245524296675\n",
      "metric =  0.0\n",
      "metric2 =  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now we can view the results like so.\n",
    "#(a future iteration of tool will improve the reporting method, for now this works)\n",
    "for keys,values in featureimportance.items():\n",
    "    print(keys)\n",
    "    print('shuffleaccuracy = ', values['shuffleaccuracy'])\n",
    "    print('baseaccuracy = ', values['baseaccuracy'])\n",
    "    print('metric = ', values['metric'])\n",
    "    print('metric2 = ', values['metric2'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I suspect the small size of this demonstration set impacted these results.\n",
    "\n",
    "#Note that for interpretting these the \"metric\" represents the impact\n",
    "#after shuffling the entire set originating from same feature and larger\n",
    "#metric implies more importance\n",
    "#and metric2 is derived after shuffling all but the current column originating from same\n",
    "#feature and smaller metric2 implies greater relative importance in that set of\n",
    "#derived features. In case you were wondering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCAn_components, PCAexcl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "derived columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['PCAcol0', 'PCAcol1', 'PCAcol2', 'PCAcol3', 'dist1_nmbr']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now if we want to apply some kind of dimensionality reduction, we can conduct \n",
    "#via Principle Component Analysis (PCA), a type of unsupervised learning.\n",
    "\n",
    "#a few defaults here is PCA is automatically performed if number of features > 50% number of rows\n",
    "#(can be turned off via ML_cmnd)\n",
    "#also the PCA type defaults to kernel PCA for all non-negative sets, sparse PCA otherwise, or regular\n",
    "#PCA if PCAn_components pass as a percent. (All via scikit PCA methods)\n",
    "\n",
    "#If there are any columns we want to exclude from PCA, we can specify in PCAexcl\n",
    "\n",
    "#We can also pass parameters to the PCA call via the ML_cmnd\n",
    "\n",
    "#Let's demosntrate, here we'll reduce to four PCA derived sets, arbitrarily excluding \n",
    "#from the transofrmation columns derived from dist1\n",
    "\n",
    "\n",
    "train, trainID, labels, \\\n",
    "validation1, validationID1, validationlabels1, \\\n",
    "validation2, validationID2, validationlabels2, \\\n",
    "test, testID, testlabels, \\\n",
    "labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
    "featureimportance, postprocess_dict = \\\n",
    "am.automunge(tiny_train, df_test = False, labels_column = 'isFraud', NArw_marker=False, \\\n",
    "             PCAn_components=4, PCAexcl=['dist1'], \\\n",
    "             pandasoutput=True, printstatus=False)\n",
    "\n",
    "print(\"derived columns\")\n",
    "list(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PCAcol0', 'PCAcol1', 'PCAcol2', 'PCAcol3', 'dist1_nmbr']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Noting that any subsequently available data can easily be consistently prepared as follows\n",
    "#with postmunge (by simply passing the postprocess_dict object returned from automunge, which\n",
    "#you did remember to save, right? If not no worries it's also possible to consistnelty process\n",
    "#by passing the test set with the exact saem original train set to automunge)\n",
    "\n",
    "test, testID, testlabels, \\\n",
    "labelsencoding_dict, finalcolumns_test = \\\n",
    "am.postmunge(postprocess_dict, tiny_test, testID_column = False, \\\n",
    "             labelscolumn = False, pandasoutput=True, printstatus=False)\n",
    "\n",
    "list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "derived columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['PCAcol0',\n",
       " 'PCAcol1',\n",
       " 'PCAcol2',\n",
       " 'PCAcol3',\n",
       " 'dist1_nmbr',\n",
       " 'addr2_26.0',\n",
       " 'addr2_60.0',\n",
       " 'addr2_87.0',\n",
       " 'card4_american express',\n",
       " 'card4_discover',\n",
       " 'card4_mastercard',\n",
       " 'card4_visa',\n",
       " 'ProductCD_C',\n",
       " 'ProductCD_H',\n",
       " 'ProductCD_R',\n",
       " 'ProductCD_S',\n",
       " 'ProductCD_W',\n",
       " 'card6_bnry']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Another useful method might be to exclude any boolean columns from the PCA\n",
    "#dimensionality reduction. We can do that with ML_cmnd by passing following:\n",
    "\n",
    "train, trainID, labels, \\\n",
    "validation1, validationID1, validationlabels1, \\\n",
    "validation2, validationID2, validationlabels2, \\\n",
    "test, testID, testlabels, \\\n",
    "labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
    "featureimportance, postprocess_dict = \\\n",
    "am.automunge(tiny_train, df_test = False, labels_column = 'isFraud', NArw_marker=False, \\\n",
    "             PCAn_components=4, PCAexcl=['dist1'], \\\n",
    "             pandasoutput=True, printstatus=False, \\\n",
    "             ML_cmnd = {'MLinfill_type':'default', \\\n",
    "                        'MLinfill_cmnd':{'RandomForestClassifier':{}, \\\n",
    "                                         'RandomForestRegressor':{}}, \\\n",
    "                        'PCA_type':'default', \\\n",
    "                        'PCA_cmnd':{'bool_PCA_excl':True}})\n",
    "\n",
    "print(\"derived columns\")\n",
    "list(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# assigncat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "derived columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['addr2_26.0',\n",
       " 'addr2_60.0',\n",
       " 'addr2_87.0',\n",
       " 'card5_exc2_bins_s<-2',\n",
       " 'card5_exc2_bins_s-21',\n",
       " 'card5_exc2_bins_s-10',\n",
       " 'card5_exc2_bins_s+01',\n",
       " 'card5_exc2_bins_s+12',\n",
       " 'card5_exc2_bins_s>+2',\n",
       " 'card4_american express',\n",
       " 'card4_discover',\n",
       " 'card4_mastercard',\n",
       " 'card4_visa',\n",
       " 'ProductCD_C',\n",
       " 'ProductCD_H',\n",
       " 'ProductCD_R',\n",
       " 'ProductCD_S',\n",
       " 'ProductCD_W',\n",
       " 'TransactionAmt_10^-1',\n",
       " 'TransactionAmt_10^0',\n",
       " 'TransactionAmt_10^1',\n",
       " 'TransactionAmt_10^2',\n",
       " 'TransactionAmt_10^3',\n",
       " 'TransactionDT_nmbr',\n",
       " 'card1_mnmx',\n",
       " 'card2_mnmx',\n",
       " 'card3_mnmx',\n",
       " 'card6_bnry',\n",
       " 'addr1_nmbr',\n",
       " 'dist1_nmbr',\n",
       " 'dist2_nmbr']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A really important part is that we don't have to defer to the automated evaluation of\n",
    "#column properties to determine processing methods, we can also assign distinct processing\n",
    "#methods to specific columns.\n",
    "\n",
    "#Now let's try assigning a few different methods to the numerical sets:\n",
    "\n",
    "#remember we're assigninbg based on the original column names before the appended suffixes\n",
    "\n",
    "#How about let's arbitrily select min-max scaling to these columns \n",
    "minmax_list = ['card1', 'card2', 'card3']\n",
    "\n",
    "#And since we previously saw that Transaction_Amt might have some skewness based on our\n",
    "#prior powertrasnform evaluation, let's set that to 'pwrs' which puts it into bins\n",
    "#based on powers of 10\n",
    "pwrs_list = ['TransactionAmt']\n",
    "\n",
    "#Let's say we don't feel the P_emaildomain is very useful, we can just delete it with null\n",
    "null_list = ['P_emaildomain']\n",
    "\n",
    "#and if there's a column we want to exclude from processiong, we can exclude with excl\n",
    "#note that any column we exclude from processing needs to be already numerically encoded\n",
    "#if we want to use any of our predictive methods like MLinfill, feature improtance, PCA\n",
    "#on other columns. (excl just passes data untouched, exc2 performs a modeinfill just in \n",
    "#case some missing points are found.)\n",
    "exc2_list = ['card5']\n",
    "\n",
    "#and we'll leave the rest to default methods\n",
    "\n",
    "train, trainID, labels, \\\n",
    "validation1, validationID1, validationlabels1, \\\n",
    "validation2, validationID2, validationlabels2, \\\n",
    "test, testID, testlabels, \\\n",
    "labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
    "featureimportance, postprocess_dict = \\\n",
    "am.automunge(tiny_train, df_test = False, labels_column = 'isFraud', NArw_marker=False, \\\n",
    "             pandasoutput=True, printstatus=False, \\\n",
    "             assigncat = {'mnmx':minmax_list, 'mnm2':[], 'mnm3':[], 'mnm4':[], 'mnm5':[], 'mnm6':[], \\\n",
    "                         'nmbr':[], 'nbr2':[], 'nbr3':[], 'MADn':[], 'MAD2':[], \\\n",
    "                         'bins':[], 'bint':[], \\\n",
    "                         'bxcx':[], 'bxc2':[], 'bxc3':[], 'bxc4':[], \\\n",
    "                         'log0':[], 'log1':[], 'pwrs':pwrs_list, \\\n",
    "                         'bnry':[], 'text':[], 'ordl':[], 'ord2':[], \\\n",
    "                         'date':[], 'dat2':[], 'wkdy':[], 'bshr':[], 'hldy':[], \\\n",
    "                         'excl':[], 'exc2':exc2_list, 'exc3':[], 'null':null_list, 'eval':[]})\n",
    "\n",
    "print(\"derived columns\")\n",
    "list(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>addr2_26.0</th>\n",
       "      <th>addr2_60.0</th>\n",
       "      <th>addr2_87.0</th>\n",
       "      <th>card5_exc2_bins_s&lt;-2</th>\n",
       "      <th>card5_exc2_bins_s-21</th>\n",
       "      <th>card5_exc2_bins_s-10</th>\n",
       "      <th>card5_exc2_bins_s+01</th>\n",
       "      <th>card5_exc2_bins_s+12</th>\n",
       "      <th>card5_exc2_bins_s&gt;+2</th>\n",
       "      <th>card4_american express</th>\n",
       "      <th>...</th>\n",
       "      <th>TransactionAmt_10^2</th>\n",
       "      <th>TransactionAmt_10^3</th>\n",
       "      <th>TransactionDT_nmbr</th>\n",
       "      <th>card1_mnmx</th>\n",
       "      <th>card2_mnmx</th>\n",
       "      <th>card3_mnmx</th>\n",
       "      <th>card6_bnry</th>\n",
       "      <th>addr1_nmbr</th>\n",
       "      <th>dist1_nmbr</th>\n",
       "      <th>dist2_nmbr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.023439</td>\n",
       "      <td>0.391159</td>\n",
       "      <td>0.763527</td>\n",
       "      <td>0.453608</td>\n",
       "      <td>0</td>\n",
       "      <td>1.025535</td>\n",
       "      <td>0.104258</td>\n",
       "      <td>-0.848042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.642753</td>\n",
       "      <td>0.663074</td>\n",
       "      <td>0.442886</td>\n",
       "      <td>0.453608</td>\n",
       "      <td>0</td>\n",
       "      <td>1.977745</td>\n",
       "      <td>-0.523147</td>\n",
       "      <td>-0.326374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.102020</td>\n",
       "      <td>0.481874</td>\n",
       "      <td>0.022044</td>\n",
       "      <td>0.453608</td>\n",
       "      <td>0</td>\n",
       "      <td>0.255207</td>\n",
       "      <td>-0.495666</td>\n",
       "      <td>-0.462592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.383530</td>\n",
       "      <td>0.855628</td>\n",
       "      <td>0.891784</td>\n",
       "      <td>0.814433</td>\n",
       "      <td>0</td>\n",
       "      <td>0.256705</td>\n",
       "      <td>-0.287355</td>\n",
       "      <td>-1.221946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.120128</td>\n",
       "      <td>0.833266</td>\n",
       "      <td>0.781563</td>\n",
       "      <td>0.453608</td>\n",
       "      <td>0</td>\n",
       "      <td>0.084023</td>\n",
       "      <td>-0.312272</td>\n",
       "      <td>-1.111596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   addr2_26.0  addr2_60.0  addr2_87.0  card5_exc2_bins_s<-2  \\\n",
       "0           0           0           1                     0   \n",
       "1           0           0           1                     0   \n",
       "2           0           0           1                     0   \n",
       "3           0           1           0                     0   \n",
       "4           0           0           1                     0   \n",
       "\n",
       "   card5_exc2_bins_s-21  card5_exc2_bins_s-10  card5_exc2_bins_s+01  \\\n",
       "0                     0                     0                     1   \n",
       "1                     0                     0                     1   \n",
       "2                     0                     0                     1   \n",
       "3                     1                     0                     0   \n",
       "4                     0                     0                     1   \n",
       "\n",
       "   card5_exc2_bins_s+12  card5_exc2_bins_s>+2  card4_american express  ...  \\\n",
       "0                     0                     0                       0  ...   \n",
       "1                     0                     0                       0  ...   \n",
       "2                     0                     0                       0  ...   \n",
       "3                     0                     0                       0  ...   \n",
       "4                     0                     0                       0  ...   \n",
       "\n",
       "   TransactionAmt_10^2  TransactionAmt_10^3  TransactionDT_nmbr  card1_mnmx  \\\n",
       "0                    1                    0            1.023439    0.391159   \n",
       "1                    1                    0            1.642753    0.663074   \n",
       "2                    0                    0           -0.102020    0.481874   \n",
       "3                    1                    0           -0.383530    0.855628   \n",
       "4                    1                    0           -0.120128    0.833266   \n",
       "\n",
       "   card2_mnmx  card3_mnmx  card6_bnry  addr1_nmbr  dist1_nmbr  dist2_nmbr  \n",
       "0    0.763527    0.453608           0    1.025535    0.104258   -0.848042  \n",
       "1    0.442886    0.453608           0    1.977745   -0.523147   -0.326374  \n",
       "2    0.022044    0.453608           0    0.255207   -0.495666   -0.462592  \n",
       "3    0.891784    0.814433           0    0.256705   -0.287355   -1.221946  \n",
       "4    0.781563    0.453608           0    0.084023   -0.312272   -1.111596  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here's what the resulting derivations look like\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# assigninfill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist1_nmbr</th>\n",
       "      <th>dist1_NArw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.523147</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.523147</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.495666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.495666</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.495666</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dist1_nmbr  dist1_NArw\n",
       "0   -0.523147           1\n",
       "1   -0.523147           0\n",
       "2   -0.495666           0\n",
       "3   -0.495666           1\n",
       "4   -0.495666           1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can also assign distinct infill methods to each column. Let's demonstrate. \n",
    "#I remember when we were looking at MLinfill that one of our columns had a few NArw\n",
    "#(rows subject to infill), let's try a different infill method on those \n",
    "\n",
    "#how about we try adjinfill which carries the value from an adjacent row\n",
    "\n",
    "#remember we're assigning columns based on their title prior to the suffix appendings\n",
    "\n",
    "train, trainID, labels, \\\n",
    "validation1, validationID1, validationlabels1, \\\n",
    "validation2, validationID2, validationlabels2, \\\n",
    "test, testID, testlabels, \\\n",
    "labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
    "featureimportance, postprocess_dict = \\\n",
    "am.automunge(tiny_train, df_test = False, labels_column = 'isFraud', \\\n",
    "             NArw_marker=True, pandasoutput=True, printstatus=False, \\\n",
    "             assigninfill = {'adjinfill':['dist1']})\n",
    "\n",
    "columns = ['dist1_nmbr', 'dist1_NArw']\n",
    "train[columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformdict and processdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list(train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['addr2_26.0',\n",
       " 'addr2_60.0',\n",
       " 'addr2_87.0',\n",
       " 'card4_american express',\n",
       " 'card4_discover',\n",
       " 'card4_mastercard',\n",
       " 'card4_visa',\n",
       " 'ProductCD_C',\n",
       " 'ProductCD_H',\n",
       " 'ProductCD_R',\n",
       " 'ProductCD_S',\n",
       " 'ProductCD_W',\n",
       " 'TransactionAmt_10^-1',\n",
       " 'TransactionAmt_10^0',\n",
       " 'TransactionAmt_10^1',\n",
       " 'TransactionAmt_10^2',\n",
       " 'TransactionAmt_10^3',\n",
       " 'TransactionDT_NArw',\n",
       " 'TransactionDT_nmbr',\n",
       " 'TransactionAmt_bxcx_nmbr',\n",
       " 'ProductCD_NArw',\n",
       " 'card1_NArw',\n",
       " 'card1_nmbr',\n",
       " 'card2_NArw',\n",
       " 'card2_nmbr',\n",
       " 'card3_NArw',\n",
       " 'card3_nmbr',\n",
       " 'card4_NArw',\n",
       " 'card5_NArw',\n",
       " 'card5_nmbr',\n",
       " 'card6_NArw',\n",
       " 'card6_bnry',\n",
       " 'addr1_NArw',\n",
       " 'addr1_nmbr',\n",
       " 'addr2_NArw',\n",
       " 'dist1_NArw',\n",
       " 'dist1_nmbr',\n",
       " 'dist2_NArw',\n",
       " 'dist2_nmbr',\n",
       " 'P_emaildomain_NArw',\n",
       " 'P_emaildomain_ordl']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trasnformdict and processdict are for more advanced users. They allow the user to design\n",
    "#custom compositions of transformations, or even incorporate their own custom defined\n",
    "#trasnformation functions into use on the platform. I won't go into full detail on these methods\n",
    "#here, I documented these a bunch in the essays which I'll link to below, but here's a taste.\n",
    "\n",
    "#Say that we have a numerical set that we want to use to apply multiple trasnformations. Let's just\n",
    "#make a few up, say that we have a set with fat tail characteristics, and we want to do multiple\n",
    "#trasnformions including a bocx-cox trasnformation, a z-score trasnformation on that output, as\n",
    "#well as a set of bins for powers of 10. Well our 'TransactionAmt' column might be a good candiate\n",
    "#for that. Let's show how.\n",
    "\n",
    "#Here we define our cusotm trasnform dict using our \"family tree primitives\"\n",
    "#Note that we always need to uyse at least one replacement primitive, if a column is intended to be left\n",
    "#intact we can include a excl trasnfo0rm as a replacement primitive.\n",
    "\n",
    "#here are the primitive definitions\n",
    "# 'parents' :           upstream / first generation / replaces column / with offspring\n",
    "# 'siblings':           upstream / first generation / supplements column / with offspring\n",
    "# 'auntsuncles' :       upstream / first generation / replaces column / no offspring\n",
    "# 'cousins' :           upstream / first generation / supplements column / no offspring\n",
    "# 'children' :          downstream parents / offspring generations / replaces column / with offspring\n",
    "# 'niecesnephews' :     downstream siblings / offspring generations / supplements column / with offspring\n",
    "# 'coworkers' :         downstream auntsuncles / offspring generations / replaces column / no offspring\n",
    "# 'friends' :           downstream cousins / offspring generations / supplements column / no offspring\n",
    "\n",
    "#So let's define our custom trasnformdict for a new root category we'll call 'cstm'\n",
    "transformdict = {'cstm' : {'parents' : ['bxcx'], \\\n",
    "                           'siblings': [], \\\n",
    "                           'auntsuncles' : [], \\\n",
    "                           'cousins' : ['pwrs'], \\\n",
    "                           'children' : [], \\\n",
    "                           'niecesnephews' : [], \\\n",
    "                           'coworkers' : [], \\\n",
    "                           'friends' : []}}\n",
    "\n",
    "#Note that since bxcx is a parent category, it will look for offspring in the primitives associated\n",
    "#with bxcx root cateogry in the library, and find there a downstream nmbr category\n",
    "\n",
    "#Note that since we are defining a new root category, we also have to define a few parameters for it\n",
    "#demonstrate here. Further detail on thsi step available in documentation. If you're not sure you might\n",
    "#want to try just copying an entry in the READ ME.\n",
    "\n",
    "#Note that since cstm is only a root cateogry and not included in the family tree primitives we don't have to\n",
    "#define a processing funciton (for the dualprocess/singleprocess/postprocess entries), we can just enter None\n",
    "\n",
    "processdict = {'cstm' : {'dualprocess' : None, \\\n",
    "                         'singleprocess' : None, \\\n",
    "                         'postprocess' : None, \\\n",
    "                         'NArowtype' : 'numeric', \\\n",
    "                         'MLinfilltype' : 'numeric', \\\n",
    "                         'labelctgy' : 'nmbr'}}\n",
    "\n",
    "#We can then pass this trasnformdict to the automunge call and assign the intended column in assigncat\n",
    "train, trainID, labels, \\\n",
    "validation1, validationID1, validationlabels1, \\\n",
    "validation2, validationID2, validationlabels2, \\\n",
    "test, testID, testlabels, \\\n",
    "labelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\n",
    "featureimportance, postprocess_dict = \\\n",
    "am.automunge(tiny_train, df_test = False, labels_column = 'isFraud', \\\n",
    "             NArw_marker=True, pandasoutput=True, printstatus=False, \\\n",
    "             assigncat = {'cstm':['TransactionAmt']}, \\\n",
    "             transformdict = transformdict, processdict = processdict)\n",
    "\n",
    "print(\"list(train)\")\n",
    "list(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and then of course use also has the ability to define their own trasnformation functions to\n",
    "#incorproate into the platform, I'll defer to the essays for that bit in the interest of brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# postmunge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________\n",
      "Begin Postmunge processing\n",
      "\n",
      "processing column:  TransactionDT\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['TransactionDT_NArw', 'TransactionDT_nmbr']\n",
      "\n",
      "processing column:  TransactionAmt\n",
      "    root category:  cstm\n",
      " returned columns:\n",
      "['TransactionAmt_bxcx_nmbr', 'TransactionAmt_10^0', 'TransactionAmt_10^2', 'TransactionAmt_10^1', 'TransactionAmt_10^3', 'TransactionAmt_10^-1']\n",
      "\n",
      "processing column:  ProductCD\n",
      "    root category:  text\n",
      " returned columns:\n",
      "['ProductCD_W', 'ProductCD_H', 'ProductCD_S', 'ProductCD_R', 'ProductCD_NArw', 'ProductCD_C']\n",
      "\n",
      "processing column:  card1\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['card1_NArw', 'card1_nmbr']\n",
      "\n",
      "processing column:  card2\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['card2_nmbr', 'card2_NArw']\n",
      "\n",
      "processing column:  card3\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['card3_nmbr', 'card3_NArw']\n",
      "\n",
      "processing column:  card4\n",
      "    root category:  text\n",
      " returned columns:\n",
      "['card4_visa', 'card4_mastercard', 'card4_discover', 'card4_american express', 'card4_NArw']\n",
      "\n",
      "processing column:  card5\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['card5_nmbr', 'card5_NArw']\n",
      "\n",
      "processing column:  card6\n",
      "    root category:  bnry\n",
      " returned columns:\n",
      "['card6_bnry', 'card6_NArw']\n",
      "\n",
      "processing column:  addr1\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['addr1_nmbr', 'addr1_NArw']\n",
      "\n",
      "processing column:  addr2\n",
      "    root category:  text\n",
      " returned columns:\n",
      "['addr2_26.0', 'addr2_60.0', 'addr2_NArw', 'addr2_87.0']\n",
      "\n",
      "processing column:  dist1\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['dist1_nmbr', 'dist1_NArw']\n",
      "\n",
      "processing column:  dist2\n",
      "    root category:  nmbr\n",
      " returned columns:\n",
      "['dist2_nmbr', 'dist2_NArw']\n",
      "\n",
      "processing column:  P_emaildomain\n",
      "    root category:  ordl\n",
      " returned columns:\n",
      "['P_emaildomain_ordl', 'P_emaildomain_NArw']\n",
      "\n",
      "infill to column:  addr2_26.0\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  addr2_60.0\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  addr2_87.0\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  card4_american express\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  card4_discover\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  card4_mastercard\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  card4_visa\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  ProductCD_C\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  ProductCD_H\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  ProductCD_R\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  ProductCD_S\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  ProductCD_W\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  TransactionAmt_10^-1\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  TransactionAmt_10^0\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  TransactionAmt_10^1\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  TransactionAmt_10^2\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  TransactionAmt_10^3\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  TransactionDT_nmbr\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  TransactionAmt_bxcx_nmbr\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  card1_nmbr\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  card2_nmbr\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  card3_nmbr\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  card5_nmbr\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  card6_bnry\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  addr1_nmbr\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  dist1_nmbr\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  dist2_nmbr\n",
      "     infill type: MLinfill\n",
      "\n",
      "infill to column:  P_emaildomain_ordl\n",
      "     infill type: MLinfill\n",
      "\n",
      "Postmunge returned column set: \n",
      "['addr2_26.0', 'addr2_60.0', 'addr2_87.0', 'card4_american express', 'card4_discover', 'card4_mastercard', 'card4_visa', 'ProductCD_C', 'ProductCD_H', 'ProductCD_R', 'ProductCD_S', 'ProductCD_W', 'TransactionAmt_10^-1', 'TransactionAmt_10^0', 'TransactionAmt_10^1', 'TransactionAmt_10^2', 'TransactionAmt_10^3', 'TransactionDT_NArw', 'TransactionDT_nmbr', 'TransactionAmt_bxcx_nmbr', 'ProductCD_NArw', 'card1_NArw', 'card1_nmbr', 'card2_NArw', 'card2_nmbr', 'card3_NArw', 'card3_nmbr', 'card4_NArw', 'card5_NArw', 'card5_nmbr', 'card6_NArw', 'card6_bnry', 'addr1_NArw', 'addr1_nmbr', 'addr2_NArw', 'dist1_NArw', 'dist1_nmbr', 'dist2_NArw', 'dist2_nmbr', 'P_emaildomain_NArw', 'P_emaildomain_ordl']\n",
      "\n",
      "_______________\n",
      "Postmunge Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#And the final bit which I'll just reiterate here is that automunge facilitates the simplest means\n",
    "#for consistent processing of subsequently available data with just a single function call\n",
    "#all you need is the postprocess_dict object returned form the original automunge call\n",
    "\n",
    "#This even works when we passed custom trasnformdict entries as was case with last postprocess_dict\n",
    "#derived in last example, however if you're defining custom trasfnormation functions for now you\n",
    "#need to save those custom function definitions are redefine in the new notewbook when applying postmunge\n",
    "\n",
    "#Here again is a demosntration of postmunge. Since the last postprocess_dict we returned\n",
    "#was with our custom transfomrations in preceding excample, the 'TransactionAmt' column will\n",
    "#be processed consistently\n",
    "\n",
    "test, testID, testlabels, \\\n",
    "labelsencoding_dict, finalcolumns_test = \\\n",
    "am.postmunge(postprocess_dict, tiny_test, testID_column = False, \\\n",
    "             labelscolumn = False, pandasoutput=True, printstatus=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['addr2_26.0',\n",
       " 'addr2_60.0',\n",
       " 'addr2_87.0',\n",
       " 'card4_american express',\n",
       " 'card4_discover',\n",
       " 'card4_mastercard',\n",
       " 'card4_visa',\n",
       " 'ProductCD_C',\n",
       " 'ProductCD_H',\n",
       " 'ProductCD_R',\n",
       " 'ProductCD_S',\n",
       " 'ProductCD_W',\n",
       " 'TransactionAmt_10^-1',\n",
       " 'TransactionAmt_10^0',\n",
       " 'TransactionAmt_10^1',\n",
       " 'TransactionAmt_10^2',\n",
       " 'TransactionAmt_10^3',\n",
       " 'TransactionDT_NArw',\n",
       " 'TransactionDT_nmbr',\n",
       " 'TransactionAmt_bxcx_nmbr',\n",
       " 'ProductCD_NArw',\n",
       " 'card1_NArw',\n",
       " 'card1_nmbr',\n",
       " 'card2_NArw',\n",
       " 'card2_nmbr',\n",
       " 'card3_NArw',\n",
       " 'card3_nmbr',\n",
       " 'card4_NArw',\n",
       " 'card5_NArw',\n",
       " 'card5_nmbr',\n",
       " 'card6_NArw',\n",
       " 'card6_bnry',\n",
       " 'addr1_NArw',\n",
       " 'addr1_nmbr',\n",
       " 'addr2_NArw',\n",
       " 'dist1_NArw',\n",
       " 'dist1_nmbr',\n",
       " 'dist2_NArw',\n",
       " 'dist2_nmbr',\n",
       " 'P_emaildomain_NArw',\n",
       " 'P_emaildomain_ordl']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great well certainly appreciate your attention and opportunity to share. I suppose next step for me is to try and hone in on my entry and perhaps get on the leaderboard. That'd be cool. \n",
    "\n",
    "Oh before I go if you'd like to see more I recently published my first collection of essays titled \"From the Diaries of John Henry\", which a big chunk included the documentation through the development of Automunge. Check it out it's all online.\n",
    "\n",
    "[turingsquared.com](http://turingsquared.com)\n",
    "\n",
    "Or for more on Automunge our website and contact info is available at \n",
    "\n",
    "[automunge.com](https://www.automunge.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
